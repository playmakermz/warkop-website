{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzJ0sVsjjfdk"
      },
      "source": [
        "# ğŸŒ World Wide Web Crawler\n",
        "\n",
        "Crawler yang bisa menjelajahi internet secara luas dengan **otomatis mencari SEED URLs**.\n",
        "\n",
        "### Cara Kerja:\n",
        "1. Input kata kunci\n",
        "2. Crawler otomatis mencari titik awal dari berbagai sumber\n",
        "3. Crawl dan ikuti link secara rekursif\n",
        "4. Simpan URL yang kontennya mengandung kata kunci"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOzY3Svzjfdm"
      },
      "source": [
        "## ğŸ“¦ Step 1: Install Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXM61Ssyjfdm",
        "outputId": "59cda097-61ac-41bc-eb24-3720e045b37a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Library berhasil diinstall!\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 -q\n",
        "print(\"âœ… Library berhasil diinstall!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1Cag0CYjfdn"
      },
      "source": [
        "## âš™ï¸ Step 2: Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ED11nIhjfdn",
        "outputId": "142d4146-4f20-47c8-8ae1-dc3e8b441364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Library berhasil diimport!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse, quote_plus\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "\n",
        "print(\"âœ… Library berhasil diimport!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBuMRsCWjfdn"
      },
      "source": [
        "## ğŸŒ Step 3: World Wide Web Crawler Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we92Q-Xgjfdn",
        "outputId": "943a7384-422e-450a-d0ef-5e5c8a945a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… WorldWideCrawler siap!\n"
          ]
        }
      ],
      "source": [
        "class WorldWideCrawler:\n",
        "    \"\"\"\n",
        "    Web Crawler yang bisa menjelajahi internet secara luas.\n",
        "    Otomatis mencari SEED URLs dari berbagai sumber.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, keywords, max_results=10, max_pages=100, max_depth=3, delay=1.0, verbose=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            keywords (str): Kata kunci yang dicari (pisah dengan koma)\n",
        "            max_results (int): Maksimal URL hasil yang match dengan kata kunci\n",
        "            max_pages (int): Maksimal halaman yang di-crawl (limit untuk tidak infinite)\n",
        "            max_depth (int): Kedalaman crawling\n",
        "            delay (float): Jeda antar request\n",
        "            verbose (bool): Tampilkan log detail\n",
        "        \"\"\"\n",
        "        # Parse keywords\n",
        "        if isinstance(keywords, str):\n",
        "            self.keywords = [kw.strip().lower() for kw in keywords.split(',') if kw.strip()]\n",
        "        else:\n",
        "            self.keywords = [kw.lower() for kw in keywords]\n",
        "\n",
        "        self.max_results = max_results\n",
        "        self.max_pages = max_pages\n",
        "        self.max_depth = max_depth\n",
        "        self.delay = delay\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Storage\n",
        "        self.visited_urls = set()\n",
        "        self.matched_urls = []\n",
        "        self.queue = deque()\n",
        "        self.discovered_domains = set()\n",
        "\n",
        "        # Stats\n",
        "        self.stats = {\n",
        "            'pages_crawled': 0,\n",
        "            'pages_matched': 0,\n",
        "            'domains_discovered': 0,\n",
        "            'errors': 0,\n",
        "            'start_time': None,\n",
        "            'end_time': None\n",
        "        }\n",
        "\n",
        "        # User agents untuk rotasi\n",
        "        self.user_agents = [\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',\n",
        "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "        ]\n",
        "\n",
        "    def log(self, message, level=\"INFO\"):\n",
        "        \"\"\"Log dengan timestamp.\"\"\"\n",
        "        if self.verbose:\n",
        "            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "            emoji = {\n",
        "                \"INFO\": \"â„¹ï¸\", \"SUCCESS\": \"âœ…\", \"ERROR\": \"âŒ\",\n",
        "                \"WARN\": \"âš ï¸\", \"DEBUG\": \"ğŸ”§\", \"FOUND\": \"ğŸ¯\",\n",
        "                \"CRAWL\": \"ğŸ•·ï¸\", \"SEED\": \"ğŸŒ±\", \"DOMAIN\": \"ğŸŒ\"\n",
        "            }\n",
        "            print(f\"   {emoji.get(level, 'â„¹ï¸')} [{timestamp}] {message}\")\n",
        "\n",
        "    def get_headers(self):\n",
        "        \"\"\"Generate random headers.\"\"\"\n",
        "        return {\n",
        "            'User-Agent': random.choice(self.user_agents),\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.9,id;q=0.8',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "        }\n",
        "\n",
        "    def discover_seed_urls(self):\n",
        "        \"\"\"\n",
        "        Otomatis mencari SEED URLs dari berbagai sumber berdasarkan kata kunci.\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'â”€' * 70}\")\n",
        "        print(\"ğŸŒ± FASE 1: MENCARI SEED URLs OTOMATIS\")\n",
        "        print(f\"{'â”€' * 70}\")\n",
        "\n",
        "        seed_urls = set()\n",
        "        keyword_query = ' '.join(self.keywords)\n",
        "        encoded_query = quote_plus(keyword_query)\n",
        "\n",
        "        # Sumber-sumber untuk mendapatkan seed URLs\n",
        "        sources = [\n",
        "            # 1. Wikipedia - hub dengan banyak external links\n",
        "            {\n",
        "                'name': 'Wikipedia Search',\n",
        "                'url': f'https://en.wikipedia.org/w/index.php?search={encoded_query}',\n",
        "                'selector': 'a',\n",
        "                'filter': lambda u: u.startswith('http') and 'wikipedia' not in u\n",
        "            },\n",
        "            # 2. Wikipedia Indonesia\n",
        "            {\n",
        "                'name': 'Wikipedia ID',\n",
        "                'url': f'https://id.wikipedia.org/w/index.php?search={encoded_query}',\n",
        "                'selector': 'a',\n",
        "                'filter': lambda u: u.startswith('http') and 'wikipedia' not in u\n",
        "            },\n",
        "            # 3. DuckDuckGo HTML\n",
        "            {\n",
        "                'name': 'DuckDuckGo',\n",
        "                'url': f'https://html.duckduckgo.com/html/?q={encoded_query}',\n",
        "                'selector': 'a.result__a',\n",
        "                'filter': lambda u: u.startswith('http')\n",
        "            },\n",
        "            # 4. Bing\n",
        "            {\n",
        "                'name': 'Bing',\n",
        "                'url': f'https://www.bing.com/search?q={encoded_query}',\n",
        "                'selector': 'li.b_algo a',\n",
        "                'filter': lambda u: u.startswith('http') and 'bing.com' not in u and 'microsoft.com' not in u\n",
        "            },\n",
        "            # 5. GitHub - untuk topik programming\n",
        "            {\n",
        "                'name': 'GitHub',\n",
        "                'url': f'https://github.com/search?q={encoded_query}&type=repositories',\n",
        "                'selector': 'a',\n",
        "                'filter': lambda u: u.startswith('http') and 'github.com' not in u\n",
        "            },\n",
        "            # 6. Reddit\n",
        "            {\n",
        "                'name': 'Reddit Search',\n",
        "                'url': f'https://www.reddit.com/search/?q={encoded_query}',\n",
        "                'selector': 'a',\n",
        "                'filter': lambda u: u.startswith('http') and 'reddit.com' not in u\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        # Hub websites Indonesia (untuk kata kunci Indonesia)\n",
        "        indonesian_hubs = [\n",
        "            'https://www.kompas.com/',\n",
        "            'https://www.detik.com/',\n",
        "            'https://www.liputan6.com/',\n",
        "            'https://www.tribunnews.com/',\n",
        "            'https://kumparan.com/',\n",
        "            'https://www.cnnindonesia.com/',\n",
        "            'https://tekno.kompas.com/',\n",
        "            'https://inet.detik.com/',\n",
        "        ]\n",
        "\n",
        "        # Programming/Tech hubs\n",
        "        tech_hubs = [\n",
        "            'https://dev.to/',\n",
        "            'https://medium.com/',\n",
        "            'https://stackoverflow.com/',\n",
        "            'https://www.freecodecamp.org/',\n",
        "            'https://www.geeksforgeeks.org/',\n",
        "            'https://realpython.com/',\n",
        "            'https://www.w3schools.com/',\n",
        "            'https://www.tutorialspoint.com/',\n",
        "            'https://www.petanikode.com/',\n",
        "            'https://www.dicoding.com/',\n",
        "            'https://www.codepolitan.com/',\n",
        "        ]\n",
        "\n",
        "        # Tambahkan hub websites sebagai seed\n",
        "        self.log(\"Menambahkan Hub Websites...\", \"SEED\")\n",
        "        for hub in indonesian_hubs + tech_hubs:\n",
        "            seed_urls.add(hub)\n",
        "            domain = urlparse(hub).netloc\n",
        "            self.discovered_domains.add(domain)\n",
        "        self.log(f\"Ditambahkan {len(indonesian_hubs) + len(tech_hubs)} hub websites\", \"SUCCESS\")\n",
        "\n",
        "        # Coba setiap sumber\n",
        "        for source in sources:\n",
        "            self.log(f\"Mencari dari {source['name']}...\", \"SEED\")\n",
        "\n",
        "            try:\n",
        "                response = requests.get(\n",
        "                    source['url'],\n",
        "                    headers=self.get_headers(),\n",
        "                    timeout=10,\n",
        "                    allow_redirects=True\n",
        "                )\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                    # Cari links\n",
        "                    links = soup.select(source['selector'])\n",
        "                    count = 0\n",
        "\n",
        "                    for link in links:\n",
        "                        href = link.get('href', '')\n",
        "\n",
        "                        # Untuk DuckDuckGo, URL ada di href langsung\n",
        "                        if href and source['filter'](href):\n",
        "                            # Bersihkan URL\n",
        "                            clean_url = href.split('?')[0] if '?' in href and 'uddg=' not in href else href\n",
        "\n",
        "                            # Ekstrak domain\n",
        "                            try:\n",
        "                                domain = urlparse(clean_url).netloc\n",
        "                                if domain and domain not in self.discovered_domains:\n",
        "                                    seed_urls.add(clean_url)\n",
        "                                    self.discovered_domains.add(domain)\n",
        "                                    count += 1\n",
        "\n",
        "                                    if count <= 3:  # Log beberapa contoh\n",
        "                                        self.log(f\"  + {domain}\", \"DOMAIN\")\n",
        "                            except:\n",
        "                                pass\n",
        "\n",
        "                    self.log(f\"Ditemukan {count} domain baru dari {source['name']}\", \"SUCCESS\")\n",
        "                else:\n",
        "                    self.log(f\"{source['name']}: Status {response.status_code}\", \"WARN\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.log(f\"{source['name']}: {type(e).__name__}\", \"ERROR\")\n",
        "\n",
        "            time.sleep(1)  # Delay antar sumber\n",
        "\n",
        "        self.stats['domains_discovered'] = len(self.discovered_domains)\n",
        "\n",
        "        print(f\"\\n   ğŸ“Š Total SEED URLs: {len(seed_urls)}\")\n",
        "        print(f\"   ğŸŒ Total Domains: {len(self.discovered_domains)}\")\n",
        "\n",
        "        return list(seed_urls)\n",
        "\n",
        "    def is_valid_url(self, url):\n",
        "        \"\"\"Cek apakah URL valid untuk di-crawl.\"\"\"\n",
        "        try:\n",
        "            parsed = urlparse(url)\n",
        "\n",
        "            if parsed.scheme not in ['http', 'https']:\n",
        "                return False\n",
        "\n",
        "            # Skip file types\n",
        "            skip_ext = ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.mp4', '.mp3',\n",
        "                       '.zip', '.rar', '.exe', '.doc', '.docx', '.xls', '.ppt']\n",
        "            if any(parsed.path.lower().endswith(ext) for ext in skip_ext):\n",
        "                return False\n",
        "\n",
        "            # Skip social media (terlalu banyak noise)\n",
        "            skip_domains = [\n",
        "                'facebook.com', 'twitter.com', 'instagram.com', 'tiktok.com',\n",
        "                'linkedin.com', 'pinterest.com', 'tumblr.com',\n",
        "                'google.com', 'bing.com', 'yahoo.com', 'duckduckgo.com',\n",
        "                'amazon.com', 'ebay.com', 'aliexpress.com',\n",
        "                'play.google.com', 'apps.apple.com'\n",
        "            ]\n",
        "            if any(domain in parsed.netloc for domain in skip_domains):\n",
        "                return False\n",
        "\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def extract_text(self, soup):\n",
        "        \"\"\"Ekstrak teks dari HTML.\"\"\"\n",
        "        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'noscript']):\n",
        "            tag.decompose()\n",
        "\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        text = re.sub(r'\\s+', ' ', text).lower()\n",
        "        return text\n",
        "\n",
        "    def check_keywords(self, text):\n",
        "        \"\"\"Cek apakah teks mengandung kata kunci.\"\"\"\n",
        "        matched = [kw for kw in self.keywords if kw in text]\n",
        "        return len(matched) > 0, matched\n",
        "\n",
        "    def extract_links(self, soup, base_url):\n",
        "        \"\"\"Ekstrak links dari halaman.\"\"\"\n",
        "        links = []\n",
        "        base_domain = urlparse(base_url).netloc\n",
        "\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            full_url = urljoin(base_url, href)\n",
        "\n",
        "            # Bersihkan URL\n",
        "            parsed = urlparse(full_url)\n",
        "            clean_url = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\".rstrip('/')\n",
        "\n",
        "            if self.is_valid_url(clean_url) and clean_url not in self.visited_urls:\n",
        "                links.append(clean_url)\n",
        "\n",
        "                # Track domain baru\n",
        "                domain = parsed.netloc\n",
        "                if domain not in self.discovered_domains:\n",
        "                    self.discovered_domains.add(domain)\n",
        "                    self.stats['domains_discovered'] = len(self.discovered_domains)\n",
        "\n",
        "        return list(set(links))\n",
        "\n",
        "    def crawl_url(self, url, depth):\n",
        "        \"\"\"Crawl satu URL.\"\"\"\n",
        "        if url in self.visited_urls:\n",
        "            return []\n",
        "\n",
        "        if len(self.matched_urls) >= self.max_results:\n",
        "            return []\n",
        "\n",
        "        if self.stats['pages_crawled'] >= self.max_pages:\n",
        "            return []\n",
        "\n",
        "        self.visited_urls.add(url)\n",
        "        self.stats['pages_crawled'] += 1\n",
        "\n",
        "        domain = urlparse(url).netloc\n",
        "        self.log(f\"[{self.stats['pages_crawled']}/{self.max_pages}] Depth:{depth} | {domain}\", \"CRAWL\")\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=self.get_headers(), timeout=10)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                self.log(f\"Status {response.status_code}\", \"WARN\")\n",
        "                return []\n",
        "\n",
        "            # Cek content type\n",
        "            content_type = response.headers.get('content-type', '')\n",
        "            if 'text/html' not in content_type:\n",
        "                return []\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            text = self.extract_text(soup)\n",
        "\n",
        "            # Cek kata kunci\n",
        "            found, matched_keywords = self.check_keywords(text)\n",
        "\n",
        "            if found:\n",
        "                self.stats['pages_matched'] += 1\n",
        "\n",
        "                title = soup.title.string if soup.title else \"No Title\"\n",
        "                title = title.strip()[:80] if title else \"No Title\"\n",
        "\n",
        "                result = {\n",
        "                    'url': url,\n",
        "                    'title': title,\n",
        "                    'domain': domain,\n",
        "                    'matched_keywords': matched_keywords,\n",
        "                    'depth': depth\n",
        "                }\n",
        "                self.matched_urls.append(result)\n",
        "\n",
        "                self.log(f\"ğŸ¯ MATCH! [{self.stats['pages_matched']}/{self.max_results}] {title[:40]}...\", \"FOUND\")\n",
        "                self.log(f\"   Keywords: {matched_keywords}\", \"SUCCESS\")\n",
        "\n",
        "            # Ekstrak links jika belum mencapai max depth\n",
        "            if depth < self.max_depth:\n",
        "                return self.extract_links(soup, url)\n",
        "\n",
        "            return []\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            self.stats['errors'] += 1\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            self.stats['errors'] += 1\n",
        "            return []\n",
        "\n",
        "    def crawl(self, custom_seeds=None):\n",
        "        \"\"\"\n",
        "        Mulai crawling.\n",
        "\n",
        "        Args:\n",
        "            custom_seeds (list): Optional - custom seed URLs. Jika None, akan auto-discover.\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"ğŸŒ WORLD WIDE WEB CRAWLER\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"ğŸ”‘ Kata Kunci     : {', '.join(self.keywords)}\")\n",
        "        print(f\"ğŸ¯ Max Hasil      : {self.max_results}\")\n",
        "        print(f\"ğŸ“„ Max Pages      : {self.max_pages}\")\n",
        "        print(f\"ğŸ“ Max Depth      : {self.max_depth}\")\n",
        "        print(f\"â±ï¸  Delay          : {self.delay}s\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        self.stats['start_time'] = datetime.now()\n",
        "\n",
        "        # Fase 1: Discover seed URLs\n",
        "        if custom_seeds:\n",
        "            seed_urls = custom_seeds\n",
        "            print(f\"\\nğŸŒ± Menggunakan {len(seed_urls)} custom seed URLs\")\n",
        "        else:\n",
        "            seed_urls = self.discover_seed_urls()\n",
        "\n",
        "        # Shuffle untuk variasi\n",
        "        random.shuffle(seed_urls)\n",
        "\n",
        "        # Fase 2: Mulai crawling\n",
        "        print(f\"\\n{'â”€' * 70}\")\n",
        "        print(\"ğŸ•·ï¸  FASE 2: CRAWLING\")\n",
        "        print(f\"{'â”€' * 70}\")\n",
        "\n",
        "        # Masukkan seed ke queue\n",
        "        for url in seed_urls:\n",
        "            if self.is_valid_url(url):\n",
        "                self.queue.append((url, 0))\n",
        "\n",
        "        # BFS Crawling\n",
        "        while self.queue:\n",
        "            # Cek stopping conditions\n",
        "            if len(self.matched_urls) >= self.max_results:\n",
        "                self.log(f\"Mencapai max hasil ({self.max_results})\", \"INFO\")\n",
        "                break\n",
        "\n",
        "            if self.stats['pages_crawled'] >= self.max_pages:\n",
        "                self.log(f\"Mencapai max pages ({self.max_pages})\", \"INFO\")\n",
        "                break\n",
        "\n",
        "            url, depth = self.queue.popleft()\n",
        "\n",
        "            if url in self.visited_urls:\n",
        "                continue\n",
        "\n",
        "            # Crawl\n",
        "            new_links = self.crawl_url(url, depth)\n",
        "\n",
        "            # Tambahkan links baru (batasi per halaman)\n",
        "            for link in new_links[:15]:\n",
        "                if link not in self.visited_urls:\n",
        "                    self.queue.append((link, depth + 1))\n",
        "\n",
        "            # Progress setiap 20 halaman\n",
        "            if self.stats['pages_crawled'] % 20 == 0:\n",
        "                print(f\"\\n   ğŸ“Š Progress: {self.stats['pages_crawled']} crawled | \"\n",
        "                      f\"{self.stats['pages_matched']} matched | \"\n",
        "                      f\"{len(self.discovered_domains)} domains | \"\n",
        "                      f\"{len(self.queue)} in queue\")\n",
        "\n",
        "            # Delay\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "        self.stats['end_time'] = datetime.now()\n",
        "\n",
        "        return self.matched_urls\n",
        "\n",
        "    def display_results(self):\n",
        "        \"\"\"Tampilkan hasil.\"\"\"\n",
        "        duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"ğŸ“‹ HASIL CRAWLING\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"ğŸ”‘ Kata Kunci         : {', '.join(self.keywords)}\")\n",
        "        print(f\"ğŸ•·ï¸  Halaman Di-crawl   : {self.stats['pages_crawled']}\")\n",
        "        print(f\"ğŸ¯ Halaman Match      : {self.stats['pages_matched']}\")\n",
        "        print(f\"ğŸŒ Domain Ditemukan   : {self.stats['domains_discovered']}\")\n",
        "        print(f\"âŒ Errors             : {self.stats['errors']}\")\n",
        "        print(f\"â±ï¸  Durasi             : {duration:.1f} detik\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        if self.matched_urls:\n",
        "            print(\"\\nğŸ“„ URL YANG MENGANDUNG KATA KUNCI:\\n\")\n",
        "            for i, item in enumerate(self.matched_urls, 1):\n",
        "                print(f\"[{i}] {item['title']}\")\n",
        "                print(f\"    ğŸ”— {item['url']}\")\n",
        "                print(f\"    ğŸ”‘ Keywords: {', '.join(item['matched_keywords'])}\")\n",
        "                print(f\"    ğŸŒ Domain: {item['domain']} | Depth: {item['depth']}\")\n",
        "                print()\n",
        "        else:\n",
        "            print(\"\\nâš ï¸ Tidak ada halaman yang mengandung kata kunci.\")\n",
        "            print(\"\\nğŸ’¡ Tips:\")\n",
        "            print(\"   - Gunakan kata kunci lebih umum\")\n",
        "            print(\"   - Naikkan max_pages\")\n",
        "            print(\"   - Naikkan max_depth\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    def save_results(self, filename=None):\n",
        "        \"\"\"Simpan hasil ke file.\"\"\"\n",
        "        if not self.matched_urls:\n",
        "            print(\"âš ï¸ Tidak ada hasil.\")\n",
        "            return None\n",
        "\n",
        "        if not filename:\n",
        "            ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            kw = '_'.join(self.keywords)[:15]\n",
        "            filename = f\"worldwide_crawl_{kw}_{ts}.txt\"\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"WORLD WIDE WEB CRAWLER - HASIL\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "            f.write(f\"Kata Kunci      : {', '.join(self.keywords)}\\n\")\n",
        "            f.write(f\"Halaman Crawled : {self.stats['pages_crawled']}\\n\")\n",
        "            f.write(f\"Halaman Match   : {self.stats['pages_matched']}\\n\")\n",
        "            f.write(f\"Domains         : {self.stats['domains_discovered']}\\n\")\n",
        "            f.write(f\"Waktu           : {datetime.now()}\\n\")\n",
        "            f.write(\"\\n\" + \"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "            for i, item in enumerate(self.matched_urls, 1):\n",
        "                f.write(f\"[{i}] {item['title']}\\n\")\n",
        "                f.write(f\"    URL: {item['url']}\\n\")\n",
        "                f.write(f\"    Keywords: {', '.join(item['matched_keywords'])}\\n\")\n",
        "                f.write(f\"    Domain: {item['domain']}\\n\\n\")\n",
        "\n",
        "        print(f\"\\nğŸ’¾ Hasil disimpan: {filename}\")\n",
        "        return filename\n",
        "\n",
        "print(\"âœ… WorldWideCrawler siap!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZWbmJZkjfdp"
      },
      "source": [
        "## ğŸš€ Step 4: Jalankan Crawler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i86dEOajfdp"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ğŸ”§ KONFIGURASI\n",
        "# ============================================================\n",
        "\n",
        "# Kata kunci yang dicari dalam KONTEN halaman\n",
        "KATA_KUNCI = \"python, pemrograman\"\n",
        "\n",
        "# Maksimal halaman yang MATCH dengan kata kunci\n",
        "MAX_HASIL = 10\n",
        "\n",
        "# Maksimal halaman yang di-crawl (limit agar tidak infinite)\n",
        "MAX_PAGES = 200\n",
        "\n",
        "# Kedalaman crawling\n",
        "MAX_DEPTH = 3\n",
        "\n",
        "# Jeda antar request (detik)\n",
        "DELAY = 0.5\n",
        "\n",
        "# Tampilkan log detail\n",
        "VERBOSE = True\n",
        "\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4Algb3hsjfdp",
        "outputId": "3e01aa7a-a543-4dc8-c278-1d86676fa46c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸŒ WORLD WIDE WEB CRAWLER\n",
            "======================================================================\n",
            "ğŸ”‘ Kata Kunci     : python, pemrograman\n",
            "ğŸ¯ Max Hasil      : 10\n",
            "ğŸ“„ Max Pages      : 200\n",
            "ğŸ“ Max Depth      : 3\n",
            "â±ï¸  Delay          : 0.5s\n",
            "======================================================================\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸŒ± FASE 1: MENCARI SEED URLs OTOMATIS\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "   ğŸŒ± [06:19:35] Menambahkan Hub Websites...\n",
            "   âœ… [06:19:35] Ditambahkan 19 hub websites\n",
            "   ğŸŒ± [06:19:35] Mencari dari Wikipedia Search...\n",
            "   ğŸŒ [06:19:35]   + foundation.wikimedia.org\n",
            "   ğŸŒ [06:19:35]   + developer.wikimedia.org\n",
            "   ğŸŒ [06:19:35]   + www.wikimedia.org\n",
            "   âœ… [06:19:35] Ditemukan 4 domain baru dari Wikipedia Search\n",
            "   ğŸŒ± [06:19:36] Mencari dari Wikipedia ID...\n",
            "   ğŸŒ [06:19:37]   + id.wikibooks.org\n",
            "   âœ… [06:19:37] Ditemukan 1 domain baru dari Wikipedia ID\n",
            "   ğŸŒ± [06:19:38] Mencari dari DuckDuckGo...\n",
            "   âš ï¸ [06:19:38] DuckDuckGo: Status 202\n",
            "   ğŸŒ± [06:19:39] Mencari dari Bing...\n",
            "   âœ… [06:19:39] Ditemukan 0 domain baru dari Bing\n",
            "   ğŸŒ± [06:19:40] Mencari dari GitHub...\n",
            "   ğŸŒ [06:19:41]   + github.blog\n",
            "   ğŸŒ [06:19:41]   + en.wikipedia.org\n",
            "   âœ… [06:19:41] Ditemukan 2 domain baru dari GitHub\n",
            "   ğŸŒ± [06:19:42] Mencari dari Reddit Search...\n",
            "   âš ï¸ [06:19:42] Reddit Search: Status 403\n",
            "\n",
            "   ğŸ“Š Total SEED URLs: 26\n",
            "   ğŸŒ Total Domains: 26\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ•·ï¸  FASE 2: CRAWLING\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "   ğŸ•·ï¸ [06:19:43] [1/200] Depth:0 | www.cnnindonesia.com\n",
            "   ğŸ•·ï¸ [06:19:43] [2/200] Depth:0 | www.mediawiki.org\n",
            "   ğŸ•·ï¸ [06:19:44] [3/200] Depth:0 | inet.detik.com\n",
            "   ğŸ•·ï¸ [06:19:46] [4/200] Depth:0 | kumparan.com\n",
            "   ğŸ•·ï¸ [06:19:48] [5/200] Depth:0 | foundation.wikimedia.org\n",
            "   ğŸ•·ï¸ [06:19:49] [6/200] Depth:0 | www.tribunnews.com\n",
            "   ğŸ•·ï¸ [06:19:50] [7/200] Depth:0 | en.wikipedia.org\n",
            "   ğŸ¯ [06:19:51] ğŸ¯ MATCH! [1/10] Python (programming language) - Wikipedi...\n",
            "   âœ… [06:19:51]    Keywords: ['python']\n",
            "   ğŸ•·ï¸ [06:19:51] [8/200] Depth:0 | www.geeksforgeeks.org\n",
            "   ğŸ¯ [06:19:51] ğŸ¯ MATCH! [2/10] GeeksforGeeks | Your All-in-One Learning...\n",
            "   âœ… [06:19:51]    Keywords: ['python']\n",
            "   ğŸ•·ï¸ [06:19:52] [9/200] Depth:0 | www.liputan6.com\n",
            "   ğŸ•·ï¸ [06:19:53] [10/200] Depth:0 | www.kompas.com\n",
            "   ğŸ•·ï¸ [06:19:54] [11/200] Depth:0 | www.wikimedia.org\n",
            "   ğŸ•·ï¸ [06:19:55] [12/200] Depth:0 | www.petanikode.com\n",
            "   ğŸ¯ [06:19:55] ğŸ¯ MATCH! [3/10] Petani Kode: Belajar Budidaya Kode...\n",
            "   âœ… [06:19:55]    Keywords: ['pemrograman']\n",
            "   ğŸ•·ï¸ [06:19:55] [13/200] Depth:0 | github.blog\n",
            "   ğŸ•·ï¸ [06:19:56] [14/200] Depth:0 | www.freecodecamp.org\n",
            "   ğŸ•·ï¸ [06:19:57] [15/200] Depth:0 | realpython.com\n",
            "   ğŸ¯ [06:19:57] ğŸ¯ MATCH! [4/10] Python Tutorials â€“ Real Python...\n",
            "   âœ… [06:19:57]    Keywords: ['python']\n",
            "   ğŸ•·ï¸ [06:19:57] [16/200] Depth:0 | developer.wikimedia.org\n",
            "   ğŸ•·ï¸ [06:19:58] [17/200] Depth:0 | dev.to\n",
            "   ğŸ•·ï¸ [06:19:59] [18/200] Depth:0 | stackoverflow.com\n",
            "   ğŸ¯ [06:19:59] ğŸ¯ MATCH! [5/10] Newest Questions - Stack Overflow...\n",
            "   âœ… [06:19:59]    Keywords: ['python']\n",
            "   ğŸ•·ï¸ [06:20:00] [19/200] Depth:0 | id.wikibooks.org\n",
            "   ğŸ•·ï¸ [06:20:01] [20/200] Depth:0 | www.codepolitan.com\n",
            "   ğŸ¯ [06:20:01] ğŸ¯ MATCH! [6/10] CODEPOLITAN: Platform Belajar Skill Tekn...\n",
            "   âœ… [06:20:01]    Keywords: ['pemrograman']\n",
            "\n",
            "   ğŸ“Š Progress: 20 crawled | 6 matched | 386 domains | 271 in queue\n",
            "   ğŸ•·ï¸ [06:20:02] [21/200] Depth:0 | tekno.kompas.com\n",
            "   ğŸ•·ï¸ [06:20:02] [22/200] Depth:0 | www.w3schools.com\n",
            "   ğŸ¯ [06:20:02] ğŸ¯ MATCH! [7/10] W3Schools Online Web Tutorials...\n",
            "   âœ… [06:20:02]    Keywords: ['python']\n",
            "   ğŸ•·ï¸ [06:20:03] [23/200] Depth:0 | www.dicoding.com\n",
            "   ğŸ¯ [06:20:04] ğŸ¯ MATCH! [8/10] Dicoding Indonesia...\n",
            "   âœ… [06:20:04]    Keywords: ['python', 'pemrograman']\n",
            "   ğŸ•·ï¸ [06:20:05] [24/200] Depth:0 | www.tutorialspoint.com\n",
            "   ğŸ¯ [06:20:05] ğŸ¯ MATCH! [9/10] Coding Practice Problems & Tutorials | T...\n",
            "   âœ… [06:20:05]    Keywords: ['python']\n",
            "   ğŸ•·ï¸ [06:20:05] [25/200] Depth:0 | medium.com\n",
            "   âš ï¸ [06:20:05] Status 403\n",
            "   ğŸ•·ï¸ [06:20:06] [26/200] Depth:0 | www.detik.com\n",
            "   ğŸ•·ï¸ [06:20:08] [27/200] Depth:1 | www.cnnindonesia.com\n",
            "   ğŸ•·ï¸ [06:20:09] [28/200] Depth:1 | connect.detik.com\n",
            "   ğŸ•·ï¸ [06:20:11] [29/200] Depth:1 | connect.detik.com\n",
            "   ğŸ•·ï¸ [06:20:13] [30/200] Depth:1 | www.cnnindonesia.com\n",
            "   ğŸ•·ï¸ [06:20:14] [31/200] Depth:1 | www.cnnindonesia.com\n",
            "   ğŸ•·ï¸ [06:20:15] [32/200] Depth:1 | www.cnnindonesia.com\n",
            "   ğŸ•·ï¸ [06:20:16] [33/200] Depth:1 | www.cnnindonesia.com\n",
            "   ğŸ•·ï¸ [06:20:17] [34/200] Depth:1 | www.cnnindonesia.com\n",
            "   ğŸ•·ï¸ [06:20:18] [35/200] Depth:1 | www.cnnindonesia.com\n",
            "   ğŸ•·ï¸ [06:20:19] [36/200] Depth:1 | www.cnnindonesia.com\n",
            "   ğŸ•·ï¸ [06:20:20] [37/200] Depth:1 | www.cnnindonesia.com\n",
            "   ğŸ•·ï¸ [06:20:21] [38/200] Depth:1 | www.cnnindonesia.com\n",
            "   ğŸ•·ï¸ [06:20:22] [39/200] Depth:1 | www.cnnindonesia.com\n",
            "   ğŸ•·ï¸ [06:20:23] [40/200] Depth:1 | www.cnnindonesia.com\n",
            "\n",
            "   ğŸ“Š Progress: 40 crawled | 9 matched | 396 domains | 406 in queue\n",
            "   ğŸ•·ï¸ [06:20:24] [41/200] Depth:1 | www.cnnindonesia.com\n",
            "   ğŸ•·ï¸ [06:20:24] [42/200] Depth:1 | www.mediawiki.org\n",
            "   ğŸ•·ï¸ [06:20:25] [43/200] Depth:1 | www.mediawiki.org\n",
            "   ğŸ¯ [06:20:25] ğŸ¯ MATCH! [10/10] Localisation - MediaWiki...\n",
            "   âœ… [06:20:25]    Keywords: ['python']\n",
            "   â„¹ï¸ [06:20:26] Mencapai max hasil (10)\n",
            "\n",
            "======================================================================\n",
            "ğŸ“‹ HASIL CRAWLING\n",
            "======================================================================\n",
            "ğŸ”‘ Kata Kunci         : python, pemrograman\n",
            "ğŸ•·ï¸  Halaman Di-crawl   : 43\n",
            "ğŸ¯ Halaman Match      : 10\n",
            "ğŸŒ Domain Ditemukan   : 403\n",
            "âŒ Errors             : 0\n",
            "â±ï¸  Durasi             : 51.2 detik\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "ğŸ“„ URL YANG MENGANDUNG KATA KUNCI:\n",
            "\n",
            "[1] Python (programming language) - Wikipedia\n",
            "    ğŸ”— https://en.wikipedia.org/wiki/Python_(programming_language)\n",
            "    ğŸ”‘ Keywords: python\n",
            "    ğŸŒ Domain: en.wikipedia.org | Depth: 0\n",
            "\n",
            "[2] GeeksforGeeks | Your All-in-One Learning Portal\n",
            "    ğŸ”— https://www.geeksforgeeks.org/\n",
            "    ğŸ”‘ Keywords: python\n",
            "    ğŸŒ Domain: www.geeksforgeeks.org | Depth: 0\n",
            "\n",
            "[3] Petani Kode: Belajar Budidaya Kode\n",
            "    ğŸ”— https://www.petanikode.com/\n",
            "    ğŸ”‘ Keywords: pemrograman\n",
            "    ğŸŒ Domain: www.petanikode.com | Depth: 0\n",
            "\n",
            "[4] Python Tutorials â€“ Real Python\n",
            "    ğŸ”— https://realpython.com/\n",
            "    ğŸ”‘ Keywords: python\n",
            "    ğŸŒ Domain: realpython.com | Depth: 0\n",
            "\n",
            "[5] Newest Questions - Stack Overflow\n",
            "    ğŸ”— https://stackoverflow.com/\n",
            "    ğŸ”‘ Keywords: python\n",
            "    ğŸŒ Domain: stackoverflow.com | Depth: 0\n",
            "\n",
            "[6] CODEPOLITAN: Platform Belajar Skill Teknologi Masa Depan\n",
            "    ğŸ”— https://www.codepolitan.com/\n",
            "    ğŸ”‘ Keywords: pemrograman\n",
            "    ğŸŒ Domain: www.codepolitan.com | Depth: 0\n",
            "\n",
            "[7] W3Schools Online Web Tutorials\n",
            "    ğŸ”— https://www.w3schools.com/\n",
            "    ğŸ”‘ Keywords: python\n",
            "    ğŸŒ Domain: www.w3schools.com | Depth: 0\n",
            "\n",
            "[8] Dicoding Indonesia\n",
            "    ğŸ”— https://www.dicoding.com/\n",
            "    ğŸ”‘ Keywords: python, pemrograman\n",
            "    ğŸŒ Domain: www.dicoding.com | Depth: 0\n",
            "\n",
            "[9] Coding Practice Problems & Tutorials | TutorialsPoint\n",
            "    ğŸ”— https://www.tutorialspoint.com/\n",
            "    ğŸ”‘ Keywords: python\n",
            "    ğŸŒ Domain: www.tutorialspoint.com | Depth: 0\n",
            "\n",
            "[10] Localisation - MediaWiki\n",
            "    ğŸ”— https://www.mediawiki.org/wiki/Special:MyLanguage/Localisation\n",
            "    ğŸ”‘ Keywords: python\n",
            "    ğŸŒ Domain: www.mediawiki.org | Depth: 1\n",
            "\n",
            "\n",
            "======================================================================\n",
            "\n",
            "ğŸ’¾ Hasil disimpan: worldwide_crawl_python_pemrogra_20260130_062026.txt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'worldwide_crawl_python_pemrogra_20260130_062026.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Jalankan!\n",
        "crawler = WorldWideCrawler(\n",
        "    keywords=KATA_KUNCI,\n",
        "    max_results=MAX_HASIL,\n",
        "    max_pages=MAX_PAGES,\n",
        "    max_depth=MAX_DEPTH,\n",
        "    delay=DELAY,\n",
        "    verbose=VERBOSE\n",
        ")\n",
        "\n",
        "# Crawl! (seed URLs akan dicari otomatis)\n",
        "results = crawler.crawl()\n",
        "\n",
        "# Tampilkan hasil\n",
        "crawler.display_results()\n",
        "\n",
        "# Simpan\n",
        "crawler.save_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuaIrZicjfdq"
      },
      "source": [
        "## ğŸ“¥ Step 5: Download Hasil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku1cLnMgjfdq"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "    import glob\n",
        "    hasil = glob.glob(\"worldwide_crawl_*.txt\")\n",
        "    if hasil:\n",
        "        files.download(hasil[-1])\n",
        "except:\n",
        "    print(\"File di direktori lokal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooNop0A2jfdq"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸ“– Penjelasan\n",
        "\n",
        "### Cara Kerja:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  FASE 1: AUTO-DISCOVER SEED URLs                        â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\n",
        "â”‚  â€¢ Wikipedia (ID & EN)                                  â”‚\n",
        "â”‚  â€¢ DuckDuckGo                                           â”‚\n",
        "â”‚  â€¢ Bing                                                 â”‚\n",
        "â”‚  â€¢ GitHub                                               â”‚\n",
        "â”‚  â€¢ Reddit                                               â”‚\n",
        "â”‚  â€¢ Hub websites (Kompas, Detik, Dev.to, dll)           â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                          â”‚\n",
        "                          â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  FASE 2: CRAWLING                                       â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\n",
        "â”‚  1. Kunjungi halaman                                    â”‚\n",
        "â”‚  2. Baca konten                                         â”‚\n",
        "â”‚  3. Cek kata kunci â”€â”€â”€â”€â”€â”€â–º MATCH? â”€â”€â–º Simpan!          â”‚\n",
        "â”‚  4. Ekstrak links                                       â”‚\n",
        "â”‚  5. Ikuti links (BFS)                                   â”‚\n",
        "â”‚  6. Ulangi sampai max_results atau max_pages           â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Parameter:\n",
        "\n",
        "| Parameter | Fungsi |\n",
        "|-----------|--------|\n",
        "| `keywords` | Kata kunci dicari di dalam konten |\n",
        "| `max_results` | Stop setelah N halaman match |\n",
        "| `max_pages` | Limit total halaman yang di-crawl |\n",
        "| `max_depth` | Seberapa dalam ikuti link |\n",
        "\n",
        "### Tips:\n",
        "- Kata kunci **umum** (\"python\") â†’ lebih banyak hasil\n",
        "- Kata kunci **spesifik** â†’ lebih sedikit tapi relevan\n",
        "- Naikkan `max_pages` untuk jangkauan lebih luas\n",
        "- Naikkan `max_depth` untuk crawl lebih dalam"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}