{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üñºÔ∏è Advanced Image Crawler & Downloader\n",
        "\n",
        "Notebook untuk crawling dan download gambar dari berbagai situs dengan dukungan **gallery-dl** untuk resolusi penuh.\n",
        "\n",
        "## ‚ú® Fitur\n",
        "- üîê Autentikasi untuk akses resolusi penuh (Pixiv, Twitter, Instagram, dll)\n",
        "- üé® Integrasi gallery-dl\n",
        "- üì∑ Download gambar resolusi tinggi\n",
        "- üîÑ Fallback ke manual crawler\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ 1. Install Dependencies\n",
        "\n",
        "Jalankan cell di bawah untuk menginstall library yang diperlukan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install requests beautifulsoup4 lxml gallery-dl -q\n",
        "\n",
        "print(\"‚úÖ Dependencies berhasil diinstall!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è 2. Import Libraries & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import shutil\n",
        "import hashlib\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from urllib.parse import urljoin, urlparse, unquote\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import Set, List, Optional, Tuple, Dict, Any\n",
        "from IPython.display import display, HTML, Image as IPImage, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß 3. Konfigurasi\n",
        "\n",
        "### 3.1 Pengaturan Dasar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# KONFIGURASI DASAR - Edit sesuai kebutuhan\n",
        "# ============================================================\n",
        "\n",
        "# Folder output untuk menyimpan gambar\n",
        "OUTPUT_DIR = \"./downloaded_images\"\n",
        "\n",
        "# Kedalaman crawling (0 = hanya halaman utama, 1-3 = ikuti link)\n",
        "MAX_DEPTH = 1\n",
        "\n",
        "# Delay antar request (detik) - untuk menghindari rate limiting\n",
        "REQUEST_DELAY = 0.5\n",
        "\n",
        "# Timeout request (detik)\n",
        "REQUEST_TIMEOUT = 30\n",
        "\n",
        "# Jumlah thread untuk parallel download\n",
        "MAX_WORKERS = 5\n",
        "\n",
        "# Gunakan gallery-dl jika tersedia\n",
        "USE_GALLERY_DL = True\n",
        "\n",
        "# Fallback ke manual crawler jika gallery-dl gagal\n",
        "FALLBACK_TO_CRAWLER = True\n",
        "\n",
        "print(\"‚úÖ Konfigurasi dasar sudah diset!\")\n",
        "print(f\"   üìÅ Output folder: {OUTPUT_DIR}\")\n",
        "print(f\"   üîç Max depth: {MAX_DEPTH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Kredensial Login (Opsional)\n",
        "\n",
        "‚ö†Ô∏è **PENTING**: Isi kredensial di bawah untuk akses resolusi penuh. Jangan share notebook yang sudah berisi kredensial!\n",
        "\n",
        "Pilih situs yang ingin Anda gunakan dan isi kredensialnya:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# KREDENSIAL - Isi sesuai kebutuhan (biarkan kosong jika tidak punya)\n",
        "# ============================================================\n",
        "\n",
        "CREDENTIALS = {\n",
        "    # ===== PIXIV =====\n",
        "    # Opsi 1: Username & Password\n",
        "    # Opsi 2: Refresh Token (lebih aman, dapatkan dengan: pip install gppt && gppt login)\n",
        "    \"pixiv\": {\n",
        "        \"username\": \"\",           # Email Pixiv\n",
        "        \"password\": \"\",           # Password Pixiv\n",
        "        \"refresh_token\": \"\",      # ATAU gunakan refresh token\n",
        "    },\n",
        "    \n",
        "    # ===== TWITTER/X =====\n",
        "    # Opsi 1: Export cookies dari browser (extension: \"Get cookies.txt\")\n",
        "    # Opsi 2: Auth token dari Developer Tools > Application > Cookies\n",
        "    \"twitter\": {\n",
        "        \"cookies_file\": \"\",       # Path ke file cookies.txt\n",
        "        \"auth_token\": \"\",         # ATAU auth_token dari cookies browser\n",
        "    },\n",
        "    \n",
        "    # ===== INSTAGRAM =====\n",
        "    # Opsi 1: Username & Password\n",
        "    # Opsi 2: Session ID dari cookies browser\n",
        "    \"instagram\": {\n",
        "        \"username\": \"\",\n",
        "        \"password\": \"\",\n",
        "        \"session_id\": \"\",         # ATAU sessionid dari cookies browser\n",
        "    },\n",
        "    \n",
        "    # ===== DEVIANTART =====\n",
        "    # Dapatkan dari: https://www.deviantart.com/developers/\n",
        "    \"deviantart\": {\n",
        "        \"client_id\": \"\",\n",
        "        \"client_secret\": \"\",\n",
        "    },\n",
        "    \n",
        "    # ===== DANBOORU =====\n",
        "    # API key dari profile settings\n",
        "    \"danbooru\": {\n",
        "        \"username\": \"\",\n",
        "        \"api_key\": \"\",\n",
        "    },\n",
        "    \n",
        "    # ===== IMGUR =====\n",
        "    # Dapatkan dari: https://api.imgur.com/oauth2/addclient\n",
        "    \"imgur\": {\n",
        "        \"client_id\": \"\",\n",
        "    },\n",
        "    \n",
        "    # ===== REDDIT =====\n",
        "    # Buat app di: https://www.reddit.com/prefs/apps\n",
        "    \"reddit\": {\n",
        "        \"client_id\": \"\",\n",
        "        \"client_secret\": \"\",\n",
        "        \"user_agent\": \"ImageCrawler/2.0\",\n",
        "    },\n",
        "}\n",
        "\n",
        "# Opsi gallery-dl per situs\n",
        "GALLERY_DL_OPTIONS = {\n",
        "    \"pixiv\": {\n",
        "        \"ugoira\": True,       # Download animasi ugoira\n",
        "        \"metadata\": True,     # Simpan metadata\n",
        "    },\n",
        "    \"twitter\": {\n",
        "        \"retweets\": False,    # Include retweets\n",
        "        \"videos\": True,       # Download video juga\n",
        "    },\n",
        "    \"instagram\": {\n",
        "        \"stories\": True,      # Download stories\n",
        "        \"highlights\": True,   # Download highlights\n",
        "        \"videos\": True,\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Kredensial sudah dikonfigurasi!\")\n",
        "\n",
        "# Tampilkan status kredensial\n",
        "print(\"\\nüìã Status Kredensial:\")\n",
        "for site, creds in CREDENTIALS.items():\n",
        "    has_creds = any(v for v in creds.values() if v)\n",
        "    status = \"‚úÖ Tersedia\" if has_creds else \"‚¨ú Kosong\"\n",
        "    print(f\"   {site.capitalize()}: {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî® 4. Core Functions\n",
        "\n",
        "Jalankan cell di bawah untuk memuat semua fungsi yang diperlukan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONSTANTS\n",
        "# ============================================================\n",
        "\n",
        "IMAGE_EXTENSIONS = {\n",
        "    '.jpg', '.jpeg', '.png', '.gif', '.webp',\n",
        "    '.bmp', '.svg', '.ico', '.tiff', '.tif'\n",
        "}\n",
        "\n",
        "SUPPORTED_SITES = {\n",
        "    'pixiv': {\n",
        "        'domains': ['pixiv.net', 'www.pixiv.net', 'i.pximg.net'],\n",
        "        'extractor': 'pixiv',\n",
        "    },\n",
        "    'twitter': {\n",
        "        'domains': ['twitter.com', 'x.com', 'pbs.twimg.com'],\n",
        "        'extractor': 'twitter',\n",
        "    },\n",
        "    'instagram': {\n",
        "        'domains': ['instagram.com', 'www.instagram.com'],\n",
        "        'extractor': 'instagram',\n",
        "    },\n",
        "    'deviantart': {\n",
        "        'domains': ['deviantart.com', 'www.deviantart.com'],\n",
        "        'extractor': 'deviantart',\n",
        "    },\n",
        "    'artstation': {\n",
        "        'domains': ['artstation.com', 'www.artstation.com'],\n",
        "        'extractor': 'artstation',\n",
        "    },\n",
        "    'danbooru': {\n",
        "        'domains': ['danbooru.donmai.us'],\n",
        "        'extractor': 'danbooru',\n",
        "    },\n",
        "    'imgur': {\n",
        "        'domains': ['imgur.com', 'i.imgur.com'],\n",
        "        'extractor': 'imgur',\n",
        "    },\n",
        "    'reddit': {\n",
        "        'domains': ['reddit.com', 'www.reddit.com', 'i.redd.it'],\n",
        "        'extractor': 'reddit',\n",
        "    },\n",
        "}\n",
        "\n",
        "# Tambahkan Bagian atas secara manual agar kamu bisa mendownload. baca dokumentasi pada gallery-dl untuk penamaan\n",
        "DEFAULT_HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Constants loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "\n",
        "def detect_site(url: str) -> Optional[str]:\n",
        "    \"\"\"Deteksi situs dari URL.\"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    domain = parsed.netloc.lower()\n",
        "    \n",
        "    for site, info in SUPPORTED_SITES.items():\n",
        "        for site_domain in info['domains']:\n",
        "            if site_domain in domain:\n",
        "                return site\n",
        "    return None\n",
        "\n",
        "\n",
        "def is_valid_image_url(url: str) -> bool:\n",
        "    \"\"\"Cek apakah URL adalah gambar.\"\"\"\n",
        "    path_lower = urlparse(url).path.lower()\n",
        "    return any(path_lower.endswith(ext) for ext in IMAGE_EXTENSIONS)\n",
        "\n",
        "\n",
        "def upgrade_to_high_res(img_url: str) -> str:\n",
        "    \"\"\"Upgrade URL ke resolusi tinggi.\"\"\"\n",
        "    # Pixiv\n",
        "    if 'pximg.net' in img_url:\n",
        "        img_url = re.sub(r'/c/\\d+x\\d+[^/]*/', '/img-original/', img_url)\n",
        "        img_url = re.sub(r'_square\\d+|_master\\d+', '', img_url)\n",
        "    # Twitter\n",
        "    elif 'twimg.com' in img_url:\n",
        "        img_url = re.sub(r'\\?.*$', '', img_url)\n",
        "        img_url += '?name=orig' if '?' not in img_url else '&name=orig'\n",
        "    # Imgur\n",
        "    elif 'imgur.com' in img_url:\n",
        "        img_url = re.sub(r'([a-zA-Z0-9]+)[smhl]\\.', r'\\1.', img_url)\n",
        "    # DeviantArt\n",
        "    elif 'wixmp.com' in img_url:\n",
        "        img_url = re.sub(r'/v1/fill/.*?/', '/', img_url)\n",
        "    \n",
        "    return img_url\n",
        "\n",
        "\n",
        "def get_filename_from_url(url: str, response=None) -> str:\n",
        "    \"\"\"Extract nama file dari URL.\"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    filename = os.path.basename(unquote(parsed.path))\n",
        "    \n",
        "    if not filename or '.' not in filename:\n",
        "        url_hash = hashlib.md5(url.encode()).hexdigest()[:12]\n",
        "        ext = '.jpg'\n",
        "        if response:\n",
        "            content_type = response.headers.get('content-type', '')\n",
        "            if 'png' in content_type: ext = '.png'\n",
        "            elif 'gif' in content_type: ext = '.gif'\n",
        "            elif 'webp' in content_type: ext = '.webp'\n",
        "        filename = f\"image_{url_hash}{ext}\"\n",
        "    \n",
        "    return re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
        "\n",
        "\n",
        "print(\"‚úÖ Helper functions loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GALLERY-DL WRAPPER\n",
        "# ============================================================\n",
        "\n",
        "class GalleryDLDownloader:\n",
        "    \"\"\"Wrapper untuk gallery-dl.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.gallery_dl_path = shutil.which('gallery-dl')\n",
        "    \n",
        "    def is_available(self) -> bool:\n",
        "        return self.gallery_dl_path is not None\n",
        "    \n",
        "    def generate_config(self, site: str) -> dict:\n",
        "        \"\"\"Generate config untuk gallery-dl.\"\"\"\n",
        "        creds = CREDENTIALS.get(site, {})\n",
        "        options = GALLERY_DL_OPTIONS.get(site, {})\n",
        "        \n",
        "        config = {\n",
        "            \"extractor\": {},\n",
        "            \"downloader\": {\"rate\": \"1M\", \"retries\": 3, \"timeout\": 30},\n",
        "        }\n",
        "        \n",
        "        if site == 'pixiv':\n",
        "            config[\"extractor\"][\"pixiv\"] = {\n",
        "                \"filename\": \"{id}_{title}_{num}.{extension}\",\n",
        "                \"directory\": [\"pixiv\", \"{user[name]}\"],\n",
        "            }\n",
        "            if creds.get('refresh_token'):\n",
        "                config[\"extractor\"][\"pixiv\"][\"refresh-token\"] = creds['refresh_token']\n",
        "            elif creds.get('username') and creds.get('password'):\n",
        "                config[\"extractor\"][\"pixiv\"][\"username\"] = creds['username']\n",
        "                config[\"extractor\"][\"pixiv\"][\"password\"] = creds['password']\n",
        "            if options.get('ugoira'):\n",
        "                config[\"extractor\"][\"pixiv\"][\"ugoira\"] = True\n",
        "        \n",
        "        elif site == 'twitter':\n",
        "            config[\"extractor\"][\"twitter\"] = {\n",
        "                \"filename\": \"{tweet_id}_{num}.{extension}\",\n",
        "                \"directory\": [\"twitter\", \"{user[name]}\"],\n",
        "                \"retweets\": options.get('retweets', False),\n",
        "                \"videos\": options.get('videos', True),\n",
        "            }\n",
        "            if creds.get('auth_token'):\n",
        "                config[\"extractor\"][\"twitter\"][\"cookies\"] = {\"auth_token\": creds['auth_token']}\n",
        "            elif creds.get('cookies_file') and os.path.exists(creds['cookies_file']):\n",
        "                config[\"extractor\"][\"twitter\"][\"cookies\"] = creds['cookies_file']\n",
        "        \n",
        "        elif site == 'instagram':\n",
        "            config[\"extractor\"][\"instagram\"] = {\n",
        "                \"filename\": \"{shortcode}_{num}.{extension}\",\n",
        "                \"directory\": [\"instagram\", \"{username}\"],\n",
        "                \"stories\": options.get('stories', True),\n",
        "                \"highlights\": options.get('highlights', True),\n",
        "            }\n",
        "            if creds.get('session_id'):\n",
        "                config[\"extractor\"][\"instagram\"][\"cookies\"] = {\"sessionid\": creds['session_id']}\n",
        "            elif creds.get('username') and creds.get('password'):\n",
        "                config[\"extractor\"][\"instagram\"][\"username\"] = creds['username']\n",
        "                config[\"extractor\"][\"instagram\"][\"password\"] = creds['password']\n",
        "        \n",
        "        elif site == 'deviantart':\n",
        "            config[\"extractor\"][\"deviantart\"] = {\n",
        "                \"filename\": \"{index}_{title}.{extension}\",\n",
        "                \"directory\": [\"deviantart\", \"{author[username]}\"],\n",
        "                \"original\": True,\n",
        "            }\n",
        "            if creds.get('client_id') and creds.get('client_secret'):\n",
        "                config[\"extractor\"][\"deviantart\"][\"client-id\"] = creds['client_id']\n",
        "                config[\"extractor\"][\"deviantart\"][\"client-secret\"] = creds['client_secret']\n",
        "        \n",
        "        elif site == 'imgur':\n",
        "            config[\"extractor\"][\"imgur\"] = {\n",
        "                \"filename\": \"{id}_{num}.{extension}\",\n",
        "                \"directory\": [\"imgur\"],\n",
        "                \"mp4\": True,\n",
        "            }\n",
        "            if creds.get('client_id'):\n",
        "                config[\"extractor\"][\"imgur\"][\"client-id\"] = creds['client_id']\n",
        "        \n",
        "        elif site == 'reddit':\n",
        "            config[\"extractor\"][\"reddit\"] = {\n",
        "                \"filename\": \"{id}_{num}.{extension}\",\n",
        "                \"directory\": [\"reddit\", \"{subreddit}\"],\n",
        "            }\n",
        "            if creds.get('client_id') and creds.get('client_secret'):\n",
        "                config[\"extractor\"][\"reddit\"][\"client-id\"] = creds['client_id']\n",
        "                config[\"extractor\"][\"reddit\"][\"client-secret\"] = creds['client_secret']\n",
        "        \n",
        "        return config\n",
        "    \n",
        "    def download(self, url: str, output_dir: str, site: str = None) -> Tuple[bool, str, list]:\n",
        "        \"\"\"Download menggunakan gallery-dl.\"\"\"\n",
        "        if not self.is_available():\n",
        "            return False, \"gallery-dl tidak tersedia\", []\n",
        "        \n",
        "        if not site:\n",
        "            site = detect_site(url)\n",
        "        \n",
        "        if not site:\n",
        "            return False, \"Situs tidak didukung\", []\n",
        "        \n",
        "        # Generate config\n",
        "        config = self.generate_config(site)\n",
        "        config_path = Path(output_dir) / \".gallery-dl-temp.conf\"\n",
        "        \n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        with open(config_path, 'w') as f:\n",
        "            json.dump(config, f, indent=2)\n",
        "        \n",
        "        # Build command\n",
        "        cmd = [\n",
        "            self.gallery_dl_path,\n",
        "            '--config', str(config_path),\n",
        "            '--dest', output_dir,\n",
        "            url\n",
        "        ]\n",
        "        \n",
        "        print(f\"üîÑ Downloading dari {site}...\")\n",
        "        print(f\"   URL: {url}\")\n",
        "        \n",
        "        try:\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
        "            config_path.unlink(missing_ok=True)\n",
        "            \n",
        "            # Parse downloaded files\n",
        "            downloaded = []\n",
        "            for line in result.stdout.split('\\n'):\n",
        "                if line.strip() and any(ext in line.lower() for ext in IMAGE_EXTENSIONS):\n",
        "                    downloaded.append(line.strip())\n",
        "            \n",
        "            if result.returncode == 0:\n",
        "                return True, f\"Berhasil download dari {site}\", downloaded\n",
        "            else:\n",
        "                return False, f\"Error: {result.stderr[:200]}\", downloaded\n",
        "        \n",
        "        except subprocess.TimeoutExpired:\n",
        "            return False, \"Timeout\", []\n",
        "        except Exception as e:\n",
        "            return False, str(e), []\n",
        "\n",
        "\n",
        "gallery_dl = GalleryDLDownloader()\n",
        "print(f\"‚úÖ GalleryDL Wrapper loaded!\")\n",
        "print(f\"   gallery-dl available: {'‚úÖ Yes' if gallery_dl.is_available() else '‚ùå No'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# MANUAL CRAWLER\n",
        "# ============================================================\n",
        "\n",
        "class ManualCrawler:\n",
        "    \"\"\"Crawler manual sebagai fallback.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update(DEFAULT_HEADERS)\n",
        "        self.visited_urls: Set[str] = set()\n",
        "        self.image_urls: Set[str] = set()\n",
        "        self.downloaded_hashes: Set[str] = set()\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset state.\"\"\"\n",
        "        self.visited_urls.clear()\n",
        "        self.image_urls.clear()\n",
        "        self.downloaded_hashes.clear()\n",
        "    \n",
        "    def crawl_page(self, url: str, depth: int = 0, max_depth: int = 1) -> Set[str]:\n",
        "        \"\"\"Crawl halaman untuk gambar.\"\"\"\n",
        "        if url in self.visited_urls or depth > max_depth:\n",
        "            return set()\n",
        "        \n",
        "        self.visited_urls.add(url)\n",
        "        found_images: Set[str] = set()\n",
        "        \n",
        "        print(f\"üîç Crawling (depth {depth}): {url[:80]}...\")\n",
        "        \n",
        "        try:\n",
        "            # Set referer\n",
        "            parsed = urlparse(url)\n",
        "            self.session.headers['Referer'] = f\"{parsed.scheme}://{parsed.netloc}/\"\n",
        "            \n",
        "            response = self.session.get(url, timeout=REQUEST_TIMEOUT)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            soup = BeautifulSoup(response.text, 'lxml')\n",
        "            \n",
        "            # 1. Tag <img>\n",
        "            for img in soup.find_all('img'):\n",
        "                for attr in ['src', 'data-src', 'data-original', 'srcset']:\n",
        "                    img_url = img.get(attr)\n",
        "                    if img_url:\n",
        "                        if attr == 'srcset':\n",
        "                            for part in img_url.split(','):\n",
        "                                src = part.strip().split()[0]\n",
        "                                found_images.add(upgrade_to_high_res(urljoin(url, src)))\n",
        "                        else:\n",
        "                            found_images.add(upgrade_to_high_res(urljoin(url, img_url)))\n",
        "            \n",
        "            # 2. Tag <a> ke gambar\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                href = link.get('href')\n",
        "                if href and is_valid_image_url(href):\n",
        "                    found_images.add(upgrade_to_high_res(urljoin(url, href)))\n",
        "            \n",
        "            # 3. Background images\n",
        "            style_pattern = r'url\\([\"\\']?([^\"\\')\\s]+)[\"\\']?\\)'\n",
        "            for element in soup.find_all(style=True):\n",
        "                matches = re.findall(style_pattern, element.get('style', ''))\n",
        "                for match in matches:\n",
        "                    full_url = urljoin(url, match)\n",
        "                    if is_valid_image_url(full_url):\n",
        "                        found_images.add(upgrade_to_high_res(full_url))\n",
        "            \n",
        "            # 4. Meta tags\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['og:image', 'twitter:image']:\n",
        "                    img_url = meta.get('content')\n",
        "                    if img_url:\n",
        "                        found_images.add(upgrade_to_high_res(urljoin(url, img_url)))\n",
        "            \n",
        "            # 5. JSON dalam script\n",
        "            for script in soup.find_all('script'):\n",
        "                if script.string:\n",
        "                    patterns = [\n",
        "                        r'\"(?:image|thumbnail|original)[Uu]rl?\"\\s*:\\s*\"([^\"]+)\"',\n",
        "                        r'\"url\"\\s*:\\s*\"(https?://[^\"]+\\.(?:jpg|jpeg|png|gif|webp))\"',\n",
        "                    ]\n",
        "                    for pattern in patterns:\n",
        "                        matches = re.findall(pattern, script.string)\n",
        "                        for match in matches:\n",
        "                            if is_valid_image_url(match):\n",
        "                                found_images.add(upgrade_to_high_res(match))\n",
        "            \n",
        "            self.image_urls.update(found_images)\n",
        "            print(f\"   ‚úÖ Ditemukan {len(found_images)} gambar\")\n",
        "            \n",
        "            # Deep crawl\n",
        "            if depth < max_depth:\n",
        "                domain = parsed.netloc\n",
        "                for link in soup.find_all('a', href=True):\n",
        "                    href = link.get('href')\n",
        "                    if href:\n",
        "                        next_url = urljoin(url, href)\n",
        "                        if urlparse(next_url).netloc == domain:\n",
        "                            time.sleep(REQUEST_DELAY)\n",
        "                            found_images.update(self.crawl_page(next_url, depth + 1, max_depth))\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error: {e}\")\n",
        "        \n",
        "        return found_images\n",
        "    \n",
        "    def download_image(self, img_url: str, output_dir: str) -> Tuple[bool, str]:\n",
        "        \"\"\"Download satu gambar.\"\"\"\n",
        "        try:\n",
        "            # Set referer\n",
        "            parsed = urlparse(img_url)\n",
        "            self.session.headers['Referer'] = f\"{parsed.scheme}://{parsed.netloc}/\"\n",
        "            \n",
        "            response = self.session.get(img_url, timeout=REQUEST_TIMEOUT, stream=True)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            content = response.content\n",
        "            content_hash = hashlib.md5(content).hexdigest()\n",
        "            \n",
        "            if content_hash in self.downloaded_hashes:\n",
        "                return False, \"Duplikat\"\n",
        "            \n",
        "            filename = get_filename_from_url(img_url, response)\n",
        "            filepath = Path(output_dir) / filename\n",
        "            \n",
        "            counter = 1\n",
        "            while filepath.exists():\n",
        "                filepath = Path(output_dir) / f\"{filepath.stem}_{counter}{filepath.suffix}\"\n",
        "                counter += 1\n",
        "            \n",
        "            with open(filepath, 'wb') as f:\n",
        "                f.write(content)\n",
        "            \n",
        "            self.downloaded_hashes.add(content_hash)\n",
        "            size_kb = len(content) / 1024\n",
        "            \n",
        "            return True, f\"{filepath.name} ({size_kb:.1f} KB)\"\n",
        "        \n",
        "        except Exception as e:\n",
        "            return False, str(e)\n",
        "    \n",
        "    def download_all(self, output_dir: str, parallel: bool = True) -> dict:\n",
        "        \"\"\"Download semua gambar yang ditemukan.\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        stats = {'downloaded': 0, 'skipped': 0, 'errors': 0}\n",
        "        total = len(self.image_urls)\n",
        "        \n",
        "        if total == 0:\n",
        "            print(\"‚ö†Ô∏è Tidak ada gambar untuk didownload\")\n",
        "            return stats\n",
        "        \n",
        "        print(f\"\\nüì• Downloading {total} gambar...\")\n",
        "        \n",
        "        if parallel and total > 1:\n",
        "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "                futures = {executor.submit(self.download_image, url, output_dir): url for url in self.image_urls}\n",
        "                \n",
        "                for i, future in enumerate(as_completed(futures), 1):\n",
        "                    success, msg = future.result()\n",
        "                    status = \"‚úÖ\" if success else \"‚¨ú\"\n",
        "                    print(f\"[{i}/{total}] {status} {msg}\")\n",
        "                    \n",
        "                    if success:\n",
        "                        stats['downloaded'] += 1\n",
        "                    else:\n",
        "                        stats['skipped'] += 1\n",
        "        else:\n",
        "            for i, url in enumerate(self.image_urls, 1):\n",
        "                success, msg = self.download_image(url, output_dir)\n",
        "                status = \"‚úÖ\" if success else \"‚¨ú\"\n",
        "                print(f\"[{i}/{total}] {status} {msg}\")\n",
        "                \n",
        "                if success:\n",
        "                    stats['downloaded'] += 1\n",
        "                else:\n",
        "                    stats['skipped'] += 1\n",
        "                \n",
        "                time.sleep(REQUEST_DELAY)\n",
        "        \n",
        "        return stats\n",
        "\n",
        "\n",
        "crawler = ManualCrawler()\n",
        "print(\"‚úÖ Manual Crawler loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# MAIN DOWNLOAD FUNCTION\n",
        "# ============================================================\n",
        "\n",
        "def download_images(url: str, output_dir: str = None, max_depth: int = None):\n",
        "    \"\"\"\n",
        "    Fungsi utama untuk download gambar.\n",
        "    \n",
        "    Args:\n",
        "        url: URL halaman/profil yang akan di-crawl\n",
        "        output_dir: Folder output (default: OUTPUT_DIR + domain)\n",
        "        max_depth: Kedalaman crawling (default: MAX_DEPTH)\n",
        "    \"\"\"\n",
        "    # Set defaults\n",
        "    if output_dir is None:\n",
        "        domain = urlparse(url).netloc\n",
        "        safe_domain = re.sub(r'[^\\w\\-.]', '_', domain)\n",
        "        output_dir = os.path.join(OUTPUT_DIR, safe_domain)\n",
        "    \n",
        "    if max_depth is None:\n",
        "        max_depth = MAX_DEPTH\n",
        "    \n",
        "    # Detect site\n",
        "    site = detect_site(url)\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"üñºÔ∏è  IMAGE CRAWLER & DOWNLOADER\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"URL: {url}\")\n",
        "    print(f\"Site: {site or 'Unknown'}\")\n",
        "    print(f\"Output: {output_dir}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    success = False\n",
        "    stats = {'downloaded': 0, 'method': 'none'}\n",
        "    \n",
        "    # Try gallery-dl first\n",
        "    if USE_GALLERY_DL and site and gallery_dl.is_available():\n",
        "        print(\"\\nüì¶ Menggunakan gallery-dl...\")\n",
        "        \n",
        "        # Check credentials\n",
        "        creds = CREDENTIALS.get(site, {})\n",
        "        has_creds = any(v for v in creds.values() if v)\n",
        "        \n",
        "        if not has_creds:\n",
        "            print(f\"‚ö†Ô∏è  Kredensial {site} tidak ditemukan (mungkin resolusi terbatas)\")\n",
        "        \n",
        "        success, message, files = gallery_dl.download(url, output_dir, site)\n",
        "        print(f\"\\n{message}\")\n",
        "        \n",
        "        if success:\n",
        "            stats['downloaded'] = len(files)\n",
        "            stats['method'] = 'gallery-dl'\n",
        "    \n",
        "    # Fallback to manual crawler\n",
        "    if not success and FALLBACK_TO_CRAWLER:\n",
        "        print(\"\\nüîß Menggunakan manual crawler...\")\n",
        "        \n",
        "        crawler.reset()\n",
        "        crawler.crawl_page(url, max_depth=max_depth)\n",
        "        \n",
        "        dl_stats = crawler.download_all(output_dir)\n",
        "        stats['downloaded'] = dl_stats['downloaded']\n",
        "        stats['method'] = 'manual'\n",
        "    \n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä RINGKASAN\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Metode: {stats['method']}\")\n",
        "    print(f\"Gambar didownload: {stats['downloaded']}\")\n",
        "    print(f\"Output: {output_dir}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # List downloaded files\n",
        "    files = list(Path(output_dir).glob('*'))\n",
        "    image_files = [f for f in files if f.suffix.lower() in IMAGE_EXTENSIONS]\n",
        "    \n",
        "    if image_files:\n",
        "        print(f\"\\nüìÅ File dalam folder ({len(image_files)} gambar):\")\n",
        "        for f in image_files[:10]:\n",
        "            size_kb = f.stat().st_size / 1024\n",
        "            print(f\"   ‚Ä¢ {f.name} ({size_kb:.1f} KB)\")\n",
        "        if len(image_files) > 10:\n",
        "            print(f\"   ... dan {len(image_files) - 10} file lainnya\")\n",
        "    \n",
        "    return stats\n",
        "\n",
        "\n",
        "print(\"‚úÖ Main function loaded!\")\n",
        "print(\"\\nüí° Gunakan: download_images('URL_ANDA')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üöÄ 5. Download Gambar\n",
        "\n",
        "### Cara Penggunaan:\n",
        "\n",
        "1. Masukkan URL di cell di bawah\n",
        "2. Jalankan cell\n",
        "3. Gambar akan tersimpan di folder `downloaded_images/[domain]/`\n",
        "\n",
        "### Contoh URL yang Didukung:\n",
        "- Pixiv: `https://www.pixiv.net/en/users/12345`\n",
        "- Twitter: `https://twitter.com/username`\n",
        "- Instagram: `https://www.instagram.com/username/`\n",
        "- DeviantArt: `https://www.deviantart.com/username`\n",
        "- ArtStation: `https://www.artstation.com/username`\n",
        "- Imgur: `https://imgur.com/a/albumid`\n",
        "- Dan website lainnya!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# MASUKKAN URL DI SINI DAN JALANKAN!\n",
        "# ============================================================\n",
        "\n",
        "URL = \"https://www.pixiv.net/en/users/86903979\"  # <-- Ganti dengan URL Anda\n",
        "\n",
        "# Jalankan download\n",
        "download_images(URL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üì• Download Multiple URLs\n",
        "\n",
        "Untuk download dari beberapa URL sekaligus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# DOWNLOAD MULTIPLE URLs\n",
        "# ============================================================\n",
        "\n",
        "URLS = [\n",
        "    # Tambahkan URL di sini\n",
        "    # \"https://www.pixiv.net/en/users/12345\",\n",
        "    # \"https://www.artstation.com/username\",\n",
        "    # \"https://example.com/gallery\",\n",
        "]\n",
        "\n",
        "# Jalankan download untuk semua URL\n",
        "if URLS:\n",
        "    for i, url in enumerate(URLS, 1):\n",
        "        print(f\"\\n{'#'*60}\")\n",
        "        print(f\"# URL {i}/{len(URLS)}\")\n",
        "        print(f\"{'#'*60}\")\n",
        "        download_images(url)\n",
        "        print(\"\\n\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Tambahkan URL ke list URLS di atas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üñºÔ∏è 6. Preview Gambar\n",
        "\n",
        "Lihat preview gambar yang sudah didownload:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PREVIEW GAMBAR YANG DIDOWNLOAD\n",
        "# ============================================================\n",
        "\n",
        "def preview_images(folder: str = OUTPUT_DIR, max_images: int = 12):\n",
        "    \"\"\"Preview gambar dalam folder.\"\"\"\n",
        "    folder = Path(folder)\n",
        "    \n",
        "    if not folder.exists():\n",
        "        print(f\"‚ùå Folder tidak ditemukan: {folder}\")\n",
        "        return\n",
        "    \n",
        "    # Find all images recursively\n",
        "    images = []\n",
        "    for ext in IMAGE_EXTENSIONS:\n",
        "        images.extend(folder.rglob(f'*{ext}'))\n",
        "        images.extend(folder.rglob(f'*{ext.upper()}'))\n",
        "    \n",
        "    if not images:\n",
        "        print(f\"‚ö†Ô∏è Tidak ada gambar di folder: {folder}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"üìÅ Folder: {folder}\")\n",
        "    print(f\"üì∑ Total gambar: {len(images)}\")\n",
        "    print(f\"üñºÔ∏è Menampilkan {min(max_images, len(images))} preview:\\n\")\n",
        "    \n",
        "    # Display images\n",
        "    displayed = 0\n",
        "    for img_path in images[:max_images]:\n",
        "        try:\n",
        "            # Skip non-displayable formats\n",
        "            if img_path.suffix.lower() in ['.svg', '.ico']:\n",
        "                continue\n",
        "            \n",
        "            display(HTML(f\"<b>{img_path.name}</b> ({img_path.stat().st_size/1024:.1f} KB)\"))\n",
        "            display(IPImage(filename=str(img_path), width=300))\n",
        "            print(\"\")\n",
        "            displayed += 1\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Tidak bisa menampilkan {img_path.name}: {e}\")\n",
        "    \n",
        "    if len(images) > max_images:\n",
        "        print(f\"\\n... dan {len(images) - max_images} gambar lainnya\")\n",
        "\n",
        "\n",
        "# Preview gambar\n",
        "preview_images(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìÇ 7. Kelola File\n",
        "\n",
        "Utilitas untuk mengelola file yang didownload:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# LIST SEMUA FOLDER DAN FILE\n",
        "# ============================================================\n",
        "\n",
        "def list_downloads(folder: str = OUTPUT_DIR):\n",
        "    \"\"\"List semua file yang didownload.\"\"\"\n",
        "    folder = Path(folder)\n",
        "    \n",
        "    if not folder.exists():\n",
        "        print(f\"‚ùå Folder tidak ada: {folder}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"üìÅ Output Directory: {folder}\\n\")\n",
        "    \n",
        "    total_files = 0\n",
        "    total_size = 0\n",
        "    \n",
        "    for subfolder in sorted(folder.iterdir()):\n",
        "        if subfolder.is_dir():\n",
        "            files = list(subfolder.glob('*'))\n",
        "            image_files = [f for f in files if f.suffix.lower() in IMAGE_EXTENSIONS]\n",
        "            folder_size = sum(f.stat().st_size for f in image_files) / (1024*1024)\n",
        "            \n",
        "            print(f\"üìÇ {subfolder.name}/\")\n",
        "            print(f\"   ‚îî‚îÄ‚îÄ {len(image_files)} gambar ({folder_size:.2f} MB)\")\n",
        "            \n",
        "            total_files += len(image_files)\n",
        "            total_size += folder_size\n",
        "    \n",
        "    # Also check root folder\n",
        "    root_files = [f for f in folder.glob('*') if f.is_file() and f.suffix.lower() in IMAGE_EXTENSIONS]\n",
        "    if root_files:\n",
        "        root_size = sum(f.stat().st_size for f in root_files) / (1024*1024)\n",
        "        print(f\"üìÑ (root): {len(root_files)} gambar ({root_size:.2f} MB)\")\n",
        "        total_files += len(root_files)\n",
        "        total_size += root_size\n",
        "    \n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"üìä Total: {total_files} gambar ({total_size:.2f} MB)\")\n",
        "\n",
        "\n",
        "list_downloads()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# HAPUS FOLDER DOWNLOAD (HATI-HATI!)\n",
        "# ============================================================\n",
        "\n",
        "def clear_downloads(folder: str = OUTPUT_DIR, confirm: bool = False):\n",
        "    \"\"\"Hapus semua file yang didownload.\"\"\"\n",
        "    folder = Path(folder)\n",
        "    \n",
        "    if not folder.exists():\n",
        "        print(f\"‚ùå Folder tidak ada: {folder}\")\n",
        "        return\n",
        "    \n",
        "    if not confirm:\n",
        "        print(f\"‚ö†Ô∏è Ini akan MENGHAPUS semua file di: {folder}\")\n",
        "        print(\"‚ö†Ô∏è Set confirm=True untuk melanjutkan\")\n",
        "        return\n",
        "    \n",
        "    import shutil\n",
        "    shutil.rmtree(folder)\n",
        "    print(f\"‚úÖ Folder {folder} telah dihapus\")\n",
        "\n",
        "\n",
        "# Uncomment baris di bawah untuk menghapus (HATI-HATI!)\n",
        "# clear_downloads(confirm=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚ÑπÔ∏è 8. Bantuan & Tips\n",
        "\n",
        "### üîê Cara Mendapatkan Kredensial:\n",
        "\n",
        "#### Pixiv (Refresh Token):\n",
        "```bash\n",
        "pip install gppt\n",
        "gppt login\n",
        "```\n",
        "Lalu copy refresh token yang muncul.\n",
        "\n",
        "#### Twitter (Auth Token):\n",
        "1. Login ke Twitter di browser\n",
        "2. Buka Developer Tools (F12)\n",
        "3. Tab Application > Cookies > twitter.com\n",
        "4. Cari `auth_token`, copy value-nya\n",
        "\n",
        "#### Instagram (Session ID):\n",
        "1. Login ke Instagram di browser\n",
        "2. Buka Developer Tools (F12)\n",
        "3. Tab Application > Cookies > instagram.com\n",
        "4. Cari `sessionid`, copy value-nya\n",
        "\n",
        "### ‚ö†Ô∏è Troubleshooting:\n",
        "\n",
        "- **gallery-dl tidak ditemukan**: Jalankan cell install di atas\n",
        "- **Login gagal**: Cek kredensial, coba refresh token/session\n",
        "- **Rate limited**: Tunggu beberapa menit, naikkan REQUEST_DELAY\n",
        "- **Gambar resolusi rendah**: Pastikan kredensial terisi\n",
        "\n",
        "### üìù Catatan:\n",
        "\n",
        "- Jangan share notebook yang berisi kredensial\n",
        "- Hormati copyright dan terms of service\n",
        "- Gunakan untuk keperluan personal saja"
      ]
    },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def compress_folder_for_manual_download(folder_path: str):\n",
    "    \"\"\"\n",
    "    Mengompres folder lokal menjadi file ZIP agar bisa didownload secara manual.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path ke folder yang ingin dikompres.\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    if not folder.exists():\n",
    "        print(f\"‚ùå Folder tidak ditemukan: {folder_path}\")\n",
    "        return\n",
    "    if not folder.is_dir():\n",
    "        print(f\"‚ùå Path yang diberikan bukan folder: {folder_path}\")\n",
    "        return\n",
    "\n",
    "    output_zip_name = f\"{folder.name}.zip\"\n",
    "    print(f\"Compressing folder '{folder_path}' to '{output_zip_name}'...\")\n",
    "    try:\n",
    "        shutil.make_archive(folder.name, 'zip', root_dir=folder.parent, base_dir=folder.name)\n",
    "        print(f\"‚úÖ Folder berhasil dikompres ke: {output_zip_name}\")\n",
    "        print(f\"‚ÑπÔ∏è Anda bisa menemukan file ini di root direktori `/content/` (di panel file Colab). Silakan download secara manual.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Terjadi error saat mengompres folder: {e}\")\n",
    "\n",
    "\n",
    "# --- CARA PENGGUNAAN ---\n",
    "# Ganti 'path/ke/folder/anda' dengan folder yang ingin Anda kompres.\n",
    "# Misalnya, untuk mengkompres semua gambar dari Folder:\n",
    "# folder_to_compress = \"downloaded_images/folder\"\n",
    "\n",
    "# Atau untuk mengkompres seluruh folder output utama:\n",
    "folder_to_compress = OUTPUT_DIR # Menggunakan variabel OUTPUT_DIR yang sudah ada\n",
    "\n",
    "compress_folder_for_manual_download(folder_to_compress)"
   ]
  },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# INFO SITUS YANG DIDUKUNG\n",
        "# ============================================================\n",
        "\n",
        "print(\"üåê Situs yang Didukung:\\n\")\n",
        "print(f\"{'Situs':<15} {'Domain':<30} {'Perlu Login?'}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "auth_info = {\n",
        "    'pixiv': 'Ya (untuk resolusi penuh)',\n",
        "    'twitter': 'Ya (untuk media)',\n",
        "    'instagram': 'Ya (untuk stories/highlights)',\n",
        "    'deviantart': 'Opsional',\n",
        "    'artstation': 'Tidak',\n",
        "    'danbooru': 'Opsional',\n",
        "    'imgur': 'Tidak',\n",
        "    'reddit': 'Opsional',\n",
        "}\n",
        "\n",
        "for site, info in SUPPORTED_SITES.items():\n",
        "    domains = ', '.join(info['domains'][:2])\n",
        "    auth = auth_info.get(site, 'Tidak diketahui')\n",
        "    print(f\"{site.capitalize():<15} {domains:<30} {auth}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
