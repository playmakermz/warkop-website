{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìù Text Expander - Document-Based Sentence Expansion\n",
        "\n",
        "**Expand sentences into paragraphs based on your novel/document vocabulary.**\n",
        "\n",
        "This notebook uses **Markov Chain** and **TF-IDF Similarity** techniques (no AI/LLM required).\n",
        "\n",
        "---\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Upload** your markdown document (novel chapters)\n",
        "2. **Process** - The system learns word patterns and relationships\n",
        "3. **Input** a sentence\n",
        "4. **Output** - Get a coherent paragraph based on your document's style\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install & Import Libraries\n",
        "\n",
        "Run this cell first to set up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries (no installation needed)\n",
        "import re\n",
        "import random\n",
        "import string\n",
        "import math\n",
        "from collections import defaultdict, Counter\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Define the Text Expander Classes\n",
        "\n",
        "This cell contains all the core logic for text processing and expansion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DocumentProcessor:\n",
        "    \"\"\"Process and clean markdown documents\"\"\"\n",
        "    \n",
        "    def __init__(self, text: str = None, filepath: str = None):\n",
        "        self.filepath = filepath\n",
        "        self.raw_text = text if text else \"\"\n",
        "        self.sentences = []\n",
        "        self.words = []\n",
        "        self.paragraphs = []\n",
        "        \n",
        "    def load_document(self) -> str:\n",
        "        \"\"\"Load markdown file\"\"\"\n",
        "        if self.filepath:\n",
        "            with open(self.filepath, 'r', encoding='utf-8') as f:\n",
        "                self.raw_text = f.read()\n",
        "        return self.raw_text\n",
        "    \n",
        "    def clean_markdown(self, text: str) -> str:\n",
        "        \"\"\"Remove markdown syntax\"\"\"\n",
        "        # Remove headers\n",
        "        text = re.sub(r'^#{1,6}\\s+', '', text, flags=re.MULTILINE)\n",
        "        # Remove bold/italic\n",
        "        text = re.sub(r'\\*{1,3}(.*?)\\*{1,3}', r'\\1', text)\n",
        "        text = re.sub(r'_{1,3}(.*?)_{1,3}', r'\\1', text)\n",
        "        # Remove links\n",
        "        text = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', text)\n",
        "        # Remove images\n",
        "        text = re.sub(r'!\\[([^\\]]*)\\]\\([^\\)]+\\)', '', text)\n",
        "        # Remove code blocks\n",
        "        text = re.sub(r'```[\\s\\S]*?```', '', text)\n",
        "        text = re.sub(r'`([^`]+)`', r'\\1', text)\n",
        "        # Remove horizontal rules\n",
        "        text = re.sub(r'^[-*_]{3,}\\s*$', '', text, flags=re.MULTILINE)\n",
        "        # Remove blockquotes\n",
        "        text = re.sub(r'^>\\s+', '', text, flags=re.MULTILINE)\n",
        "        # Remove list markers\n",
        "        text = re.sub(r'^[\\s]*[-*+]\\s+', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'^[\\s]*\\d+\\.\\s+', '', text, flags=re.MULTILINE)\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    def extract_sentences(self, text: str) -> list:\n",
        "        \"\"\"Extract sentences from text\"\"\"\n",
        "        # Handle common abbreviations to avoid false splits\n",
        "        text = re.sub(r'Mr\\.', 'Mr', text)\n",
        "        text = re.sub(r'Mrs\\.', 'Mrs', text)\n",
        "        text = re.sub(r'Ms\\.', 'Ms', text)\n",
        "        text = re.sub(r'Dr\\.', 'Dr', text)\n",
        "        text = re.sub(r'Prof\\.', 'Prof', text)\n",
        "        text = re.sub(r'St\\.', 'St', text)\n",
        "        \n",
        "        # Split on sentence-ending punctuation\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        # Clean and filter empty sentences\n",
        "        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 15]\n",
        "        return sentences\n",
        "    \n",
        "    def extract_words(self, text: str) -> list:\n",
        "        \"\"\"Extract words from text (English optimized)\"\"\"\n",
        "        # Extract words including contractions\n",
        "        words = re.findall(r\"\\b[a-zA-Z]+(?:'[a-zA-Z]+)?\\b\", text.lower())\n",
        "        return words\n",
        "    \n",
        "    def extract_paragraphs(self, text: str) -> list:\n",
        "        \"\"\"Extract paragraphs from text\"\"\"\n",
        "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "        paragraphs = [p.strip() for p in paragraphs if p.strip() and len(p.strip()) > 50]\n",
        "        return paragraphs\n",
        "    \n",
        "    def process(self) -> dict:\n",
        "        \"\"\"Process the complete document\"\"\"\n",
        "        if self.filepath and not self.raw_text:\n",
        "            self.load_document()\n",
        "        \n",
        "        cleaned = self.clean_markdown(self.raw_text)\n",
        "        \n",
        "        self.sentences = self.extract_sentences(cleaned)\n",
        "        self.words = self.extract_words(cleaned)\n",
        "        self.paragraphs = self.extract_paragraphs(cleaned)\n",
        "        \n",
        "        return {\n",
        "            'sentences': self.sentences,\n",
        "            'words': self.words,\n",
        "            'paragraphs': self.paragraphs,\n",
        "            'word_count': len(self.words),\n",
        "            'sentence_count': len(self.sentences),\n",
        "            'unique_words': len(set(self.words))\n",
        "        }\n",
        "\n",
        "\n",
        "class MarkovChain:\n",
        "    \"\"\"Markov Chain for text generation\"\"\"\n",
        "    \n",
        "    def __init__(self, order: int = 2):\n",
        "        self.order = order  # N-gram order\n",
        "        self.chain = defaultdict(list)\n",
        "        self.starters = []  # Sentence starting words\n",
        "        \n",
        "    def train(self, sentences: list):\n",
        "        \"\"\"Train the model from sentences\"\"\"\n",
        "        for sentence in sentences:\n",
        "            words = sentence.split()\n",
        "            if len(words) < self.order + 1:\n",
        "                continue\n",
        "            \n",
        "            # Store starter (sentence beginning words)\n",
        "            starter = tuple(words[:self.order])\n",
        "            self.starters.append(starter)\n",
        "            \n",
        "            # Build chain\n",
        "            for i in range(len(words) - self.order):\n",
        "                key = tuple(words[i:i + self.order])\n",
        "                next_word = words[i + self.order]\n",
        "                self.chain[key].append(next_word)\n",
        "    \n",
        "    def generate(self, seed_words: list = None, max_words: int = 50) -> str:\n",
        "        \"\"\"Generate text from seed words\"\"\"\n",
        "        if seed_words and len(seed_words) >= self.order:\n",
        "            current = self._find_matching_key(seed_words)\n",
        "        else:\n",
        "            if not self.starters:\n",
        "                return \"\"\n",
        "            current = random.choice(self.starters)\n",
        "        \n",
        "        if not current:\n",
        "            return \"\"\n",
        "            \n",
        "        result = list(current)\n",
        "        \n",
        "        for _ in range(max_words - self.order):\n",
        "            if current not in self.chain:\n",
        "                current = self._find_similar_key(current)\n",
        "                if not current:\n",
        "                    break\n",
        "            \n",
        "            next_words = self.chain.get(current, [])\n",
        "            if not next_words:\n",
        "                break\n",
        "                \n",
        "            next_word = random.choice(next_words)\n",
        "            result.append(next_word)\n",
        "            current = tuple(result[-self.order:])\n",
        "            \n",
        "            # Stop at sentence-ending punctuation\n",
        "            if next_word.endswith(('.', '!', '?')):\n",
        "                break\n",
        "        \n",
        "        return ' '.join(result)\n",
        "    \n",
        "    def _find_matching_key(self, words: list) -> tuple:\n",
        "        \"\"\"Find a key matching the given words\"\"\"\n",
        "        words_lower = [w.lower() for w in words]\n",
        "        \n",
        "        # Try exact match\n",
        "        for i in range(len(words_lower) - self.order + 1):\n",
        "            key = tuple(words_lower[i:i + self.order])\n",
        "            if key in self.chain:\n",
        "                return key\n",
        "        \n",
        "        # Try partial match\n",
        "        for key in self.chain.keys():\n",
        "            key_lower = tuple(w.lower() for w in key)\n",
        "            if any(w in key_lower for w in words_lower):\n",
        "                return key\n",
        "        \n",
        "        return random.choice(self.starters) if self.starters else None\n",
        "    \n",
        "    def _find_similar_key(self, current: tuple) -> tuple:\n",
        "        \"\"\"Find a similar key\"\"\"\n",
        "        current_lower = tuple(w.lower() for w in current)\n",
        "        \n",
        "        for key in self.chain.keys():\n",
        "            key_lower = tuple(w.lower() for w in key)\n",
        "            if any(w in current_lower for w in key_lower):\n",
        "                return key\n",
        "        \n",
        "        return random.choice(self.starters) if self.starters else None\n",
        "\n",
        "\n",
        "class SimilarityFinder:\n",
        "    \"\"\"Find similar sentences/paragraphs using TF-IDF\"\"\"\n",
        "    \n",
        "    # Common English stop words to reduce noise\n",
        "    STOP_WORDS = {\n",
        "        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
        "        'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',\n",
        "        'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',\n",
        "        'could', 'should', 'may', 'might', 'must', 'shall', 'can', 'need',\n",
        "        'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it',\n",
        "        'we', 'they', 'what', 'which', 'who', 'whom', 'whose', 'where',\n",
        "        'when', 'why', 'how', 'all', 'each', 'every', 'both', 'few', 'more',\n",
        "        'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',\n",
        "        'same', 'so', 'than', 'too', 'very', 'just', 'also', 'now', 'here',\n",
        "        'there', 'then', 'once', 'her', 'his', 'him', 'my', 'your', 'our',\n",
        "        'their', 'its', 'me', 'us', 'them', 'into', 'through', 'during',\n",
        "        'before', 'after', 'above', 'below', 'between', 'under', 'again',\n",
        "        'further', 'while', 'about', 'against', 'being', 'having', 'doing'\n",
        "    }\n",
        "    \n",
        "    def __init__(self, sentences: list, paragraphs: list):\n",
        "        self.sentences = sentences\n",
        "        self.paragraphs = paragraphs\n",
        "        self.word_idf = {}\n",
        "        self._calculate_idf()\n",
        "    \n",
        "    def _calculate_idf(self):\n",
        "        \"\"\"Calculate IDF for each word\"\"\"\n",
        "        doc_count = len(self.sentences)\n",
        "        word_doc_count = defaultdict(int)\n",
        "        \n",
        "        for sentence in self.sentences:\n",
        "            words = set(re.findall(r\"\\b[a-zA-Z]+(?:'[a-zA-Z]+)?\\b\", sentence.lower()))\n",
        "            # Filter stop words\n",
        "            words = words - self.STOP_WORDS\n",
        "            for word in words:\n",
        "                word_doc_count[word] += 1\n",
        "        \n",
        "        for word, count in word_doc_count.items():\n",
        "            self.word_idf[word] = math.log(doc_count / (1 + count))\n",
        "    \n",
        "    def _get_tfidf_vector(self, text: str) -> dict:\n",
        "        \"\"\"Get TF-IDF vector for text\"\"\"\n",
        "        words = re.findall(r\"\\b[a-zA-Z]+(?:'[a-zA-Z]+)?\\b\", text.lower())\n",
        "        # Filter stop words\n",
        "        words = [w for w in words if w not in self.STOP_WORDS]\n",
        "        word_count = Counter(words)\n",
        "        total_words = len(words)\n",
        "        \n",
        "        vector = {}\n",
        "        for word, count in word_count.items():\n",
        "            tf = count / total_words if total_words > 0 else 0\n",
        "            idf = self.word_idf.get(word, 0)\n",
        "            vector[word] = tf * idf\n",
        "        \n",
        "        return vector\n",
        "    \n",
        "    def _cosine_similarity(self, vec1: dict, vec2: dict) -> float:\n",
        "        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
        "        common_words = set(vec1.keys()) & set(vec2.keys())\n",
        "        \n",
        "        if not common_words:\n",
        "            return 0.0\n",
        "        \n",
        "        dot_product = sum(vec1[w] * vec2[w] for w in common_words)\n",
        "        norm1 = math.sqrt(sum(v ** 2 for v in vec1.values()))\n",
        "        norm2 = math.sqrt(sum(v ** 2 for v in vec2.values()))\n",
        "        \n",
        "        if norm1 == 0 or norm2 == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        return dot_product / (norm1 * norm2)\n",
        "    \n",
        "    def find_similar_sentences(self, query: str, top_n: int = 5) -> list:\n",
        "        \"\"\"Find sentences most similar to the query\"\"\"\n",
        "        query_vec = self._get_tfidf_vector(query)\n",
        "        \n",
        "        similarities = []\n",
        "        for sentence in self.sentences:\n",
        "            sent_vec = self._get_tfidf_vector(sentence)\n",
        "            sim = self._cosine_similarity(query_vec, sent_vec)\n",
        "            similarities.append((sentence, sim))\n",
        "        \n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "        return similarities[:top_n]\n",
        "    \n",
        "    def find_similar_paragraphs(self, query: str, top_n: int = 3) -> list:\n",
        "        \"\"\"Find paragraphs most similar to the query\"\"\"\n",
        "        query_vec = self._get_tfidf_vector(query)\n",
        "        \n",
        "        similarities = []\n",
        "        for paragraph in self.paragraphs:\n",
        "            para_vec = self._get_tfidf_vector(paragraph)\n",
        "            sim = self._cosine_similarity(query_vec, para_vec)\n",
        "            similarities.append((paragraph, sim))\n",
        "        \n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "        return similarities[:top_n]\n",
        "\n",
        "\n",
        "class TextExpander:\n",
        "    \"\"\"Main class for expanding sentences into paragraphs\"\"\"\n",
        "    \n",
        "    def __init__(self, text: str = None, filepath: str = None):\n",
        "        self.text = text\n",
        "        self.filepath = filepath\n",
        "        self.processor = None\n",
        "        self.markov = None\n",
        "        self.similarity = None\n",
        "        self.data = None\n",
        "        \n",
        "    def initialize(self):\n",
        "        \"\"\"Initialize all components\"\"\"\n",
        "        print(\"üìñ Loading document...\")\n",
        "        \n",
        "        # Process document\n",
        "        self.processor = DocumentProcessor(text=self.text, filepath=self.filepath)\n",
        "        self.data = self.processor.process()\n",
        "        \n",
        "        print(f\"   ‚úì {self.data['sentence_count']} sentences found\")\n",
        "        print(f\"   ‚úì {self.data['word_count']} total words\")\n",
        "        print(f\"   ‚úì {self.data['unique_words']} unique words\")\n",
        "        print(f\"   ‚úì {len(self.data['paragraphs'])} paragraphs\")\n",
        "        \n",
        "        # Initialize Markov Chain\n",
        "        print(\"\\nüîó Building Markov Chain...\")\n",
        "        self.markov = MarkovChain(order=2)\n",
        "        self.markov.train(self.data['sentences'])\n",
        "        print(f\"   ‚úì Model trained with {len(self.markov.chain)} transitions\")\n",
        "        \n",
        "        # Initialize Similarity Finder\n",
        "        print(\"\\nüîç Building similarity index...\")\n",
        "        self.similarity = SimilarityFinder(\n",
        "            self.data['sentences'], \n",
        "            self.data['paragraphs']\n",
        "        )\n",
        "        print(f\"   ‚úì IDF calculated for {len(self.similarity.word_idf)} words\")\n",
        "        \n",
        "        print(\"\\n‚úÖ System ready!\\n\")\n",
        "    \n",
        "    def expand(self, input_sentence: str, method: str = 'hybrid', \n",
        "               num_sentences: int = 4) -> str:\n",
        "        \"\"\"\n",
        "        Expand input sentence into a paragraph\n",
        "        \n",
        "        Args:\n",
        "            input_sentence: Input sentence\n",
        "            method: Method to use ('markov', 'similarity', 'hybrid')\n",
        "            num_sentences: Number of sentences in output\n",
        "            \n",
        "        Returns:\n",
        "            Expanded paragraph\n",
        "        \"\"\"\n",
        "        if method == 'markov':\n",
        "            return self._expand_markov(input_sentence, num_sentences)\n",
        "        elif method == 'similarity':\n",
        "            return self._expand_similarity(input_sentence, num_sentences)\n",
        "        else:  # hybrid\n",
        "            return self._expand_hybrid(input_sentence, num_sentences)\n",
        "    \n",
        "    def _expand_markov(self, input_sentence: str, num_sentences: int) -> str:\n",
        "        \"\"\"Expand using pure Markov Chain\"\"\"\n",
        "        words = input_sentence.split()\n",
        "        sentences = [input_sentence]\n",
        "        used_content = set()\n",
        "        used_content.add(input_sentence.lower().strip())\n",
        "        \n",
        "        attempts = 0\n",
        "        max_attempts = num_sentences * 8\n",
        "        \n",
        "        while len(sentences) < num_sentences and attempts < max_attempts:\n",
        "            attempts += 1\n",
        "            \n",
        "            if attempts % 2 == 0 and len(sentences) > 1:\n",
        "                random_sent = random.choice(sentences)\n",
        "                words = random_sent.split()[-3:]\n",
        "            else:\n",
        "                words = sentences[-1].split()[-2:]\n",
        "            \n",
        "            generated = self.markov.generate(words, max_words=35)\n",
        "            \n",
        "            if generated:\n",
        "                normalized = generated.lower().strip()\n",
        "                \n",
        "                is_duplicate = False\n",
        "                for used in used_content:\n",
        "                    gen_words = set(normalized.split())\n",
        "                    used_words = set(used.split())\n",
        "                    if gen_words and used_words:\n",
        "                        overlap = len(gen_words & used_words) / min(len(gen_words), len(used_words))\n",
        "                        if overlap > 0.6:\n",
        "                            is_duplicate = True\n",
        "                            break\n",
        "                \n",
        "                if not is_duplicate and len(generated.split()) > 4:\n",
        "                    sentences.append(generated)\n",
        "                    used_content.add(normalized)\n",
        "        \n",
        "        return ' '.join(sentences)\n",
        "    \n",
        "    def _expand_similarity(self, input_sentence: str, num_sentences: int) -> str:\n",
        "        \"\"\"Expand using similarity search\"\"\"\n",
        "        similar = self.similarity.find_similar_sentences(input_sentence, num_sentences * 2)\n",
        "        \n",
        "        result = [input_sentence]\n",
        "        used_words = set(input_sentence.lower().split())\n",
        "        \n",
        "        for sentence, score in similar:\n",
        "            if len(result) >= num_sentences:\n",
        "                break\n",
        "            \n",
        "            sent_words = set(sentence.lower().split())\n",
        "            overlap = len(used_words & sent_words) / len(sent_words) if sent_words else 1\n",
        "            \n",
        "            if overlap < 0.7 and score > 0.1:\n",
        "                result.append(sentence)\n",
        "                used_words.update(sent_words)\n",
        "        \n",
        "        return ' '.join(result)\n",
        "    \n",
        "    def _expand_hybrid(self, input_sentence: str, num_sentences: int) -> str:\n",
        "        \"\"\"Expand using combination of Markov and Similarity\"\"\"\n",
        "        result = [input_sentence]\n",
        "        used_content = set()\n",
        "        used_content.add(input_sentence.lower().strip())\n",
        "        \n",
        "        # Find relevant paragraphs and sentences for context\n",
        "        similar_paras = self.similarity.find_similar_paragraphs(input_sentence, 3)\n",
        "        similar_sents = self.similarity.find_similar_sentences(input_sentence, num_sentences * 3)\n",
        "        \n",
        "        # Gather candidate sentences from similarity\n",
        "        candidate_sentences = []\n",
        "        for sent, score in similar_sents:\n",
        "            normalized = sent.lower().strip()\n",
        "            if normalized not in used_content and score > 0.05:\n",
        "                candidate_sentences.append((sent, score))\n",
        "        \n",
        "        # Use words from relevant paragraphs as additional seeds\n",
        "        context_words = []\n",
        "        for para, _ in similar_paras:\n",
        "            words = re.findall(r\"\\b[a-zA-Z]+(?:'[a-zA-Z]+)?\\b\", para.lower())\n",
        "            context_words.extend(words[:30])\n",
        "        \n",
        "        # Generate sentences with combination of Markov and Similarity\n",
        "        attempts = 0\n",
        "        max_attempts = num_sentences * 5\n",
        "        \n",
        "        while len(result) < num_sentences and attempts < max_attempts:\n",
        "            attempts += 1\n",
        "            generated = None\n",
        "            \n",
        "            # Alternate between Markov and Similarity\n",
        "            if attempts % 3 != 0:\n",
        "                if context_words:\n",
        "                    idx = random.randint(0, max(0, len(context_words) - 3))\n",
        "                    temp_seed = context_words[idx:idx + 2]\n",
        "                else:\n",
        "                    temp_seed = result[-1].split()[-3:]\n",
        "                \n",
        "                generated = self.markov.generate(temp_seed, max_words=30)\n",
        "            else:\n",
        "                if candidate_sentences:\n",
        "                    generated, _ = candidate_sentences.pop(0)\n",
        "            \n",
        "            if generated:\n",
        "                normalized = generated.lower().strip()\n",
        "                is_duplicate = False\n",
        "                for used in used_content:\n",
        "                    gen_words = set(normalized.split())\n",
        "                    used_words = set(used.split())\n",
        "                    if gen_words and used_words:\n",
        "                        overlap = len(gen_words & used_words) / min(len(gen_words), len(used_words))\n",
        "                        if overlap > 0.7:\n",
        "                            is_duplicate = True\n",
        "                            break\n",
        "                \n",
        "                if not is_duplicate and len(generated.split()) > 3:\n",
        "                    result.append(generated)\n",
        "                    used_content.add(normalized)\n",
        "        \n",
        "        # If still not enough, force add from similarity\n",
        "        while len(result) < num_sentences and candidate_sentences:\n",
        "            sent, _ = candidate_sentences.pop(0)\n",
        "            if sent.lower().strip() not in used_content:\n",
        "                result.append(sent)\n",
        "                used_content.add(sent.lower().strip())\n",
        "        \n",
        "        return ' '.join(result)\n",
        "    \n",
        "    def analyze_input(self, input_sentence: str) -> dict:\n",
        "        \"\"\"Analyze input and show information\"\"\"\n",
        "        words = re.findall(r\"\\b[a-zA-Z]+(?:'[a-zA-Z]+)?\\b\", input_sentence.lower())\n",
        "        \n",
        "        doc_words = set(self.data['words'])\n",
        "        matching_words = [w for w in words if w in doc_words]\n",
        "        \n",
        "        similar = self.similarity.find_similar_sentences(input_sentence, 3)\n",
        "        \n",
        "        return {\n",
        "            'input_words': len(words),\n",
        "            'matching_words': matching_words,\n",
        "            'match_ratio': len(matching_words) / len(words) if words else 0,\n",
        "            'similar_sentences': similar\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"‚úÖ Text Expander classes defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Upload Your Markdown Document\n",
        "\n",
        "Upload your novel/document in `.md` format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload your markdown file\n",
        "print(\"üì§ Please upload your markdown (.md) document:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file content\n",
        "document_text = \"\"\n",
        "filename = \"\"\n",
        "\n",
        "for fn, content in uploaded.items():\n",
        "    filename = fn\n",
        "    document_text = content.decode('utf-8')\n",
        "    print(f\"\\n‚úÖ File '{fn}' uploaded successfully!\")\n",
        "    print(f\"   Size: {len(content):,} bytes\")\n",
        "    print(f\"   Characters: {len(document_text):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Initialize the Text Expander\n",
        "\n",
        "Process your document and build the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Text Expander with your document\n",
        "expander = TextExpander(text=document_text)\n",
        "expander.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Expand Sentences! üöÄ\n",
        "\n",
        "Now you can expand sentences into paragraphs.\n",
        "\n",
        "**Available Methods:**\n",
        "- `'hybrid'` (default) - Best results, combines both techniques\n",
        "- `'markov'` - Uses word transition patterns\n",
        "- `'similarity'` - Finds similar sentences from document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title üñäÔ∏è Enter Your Sentence { run: \"auto\", display-mode: \"form\" }\n",
        "\n",
        "input_sentence = \"The sun rose over the mountains\"  #@param {type:\"string\"}\n",
        "method = \"hybrid\"  #@param [\"hybrid\", \"markov\", \"similarity\"]\n",
        "num_sentences = 4  #@param {type:\"slider\", min:2, max:8, step:1}\n",
        "\n",
        "print(f\"üìù Input: {input_sentence}\")\n",
        "print(f\"‚öôÔ∏è  Method: {method}\")\n",
        "print(f\"üìä Output sentences: {num_sentences}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "result = expander.expand(input_sentence, method=method, num_sentences=num_sentences)\n",
        "\n",
        "print(\"\\nüìÑ OUTPUT:\")\n",
        "print(\"-\"*60)\n",
        "# Pretty print the paragraph\n",
        "import textwrap\n",
        "wrapped = textwrap.fill(result, width=70)\n",
        "print(wrapped)\n",
        "print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Analyze Your Input (Optional)\n",
        "\n",
        "See how well your input matches the document vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title üîç Analyze Input Sentence { run: \"auto\", display-mode: \"form\" }\n",
        "\n",
        "analyze_sentence = \"The hero walked through the forest\"  #@param {type:\"string\"}\n",
        "\n",
        "analysis = expander.analyze_input(analyze_sentence)\n",
        "\n",
        "print(f\"üìä Analysis for: '{analyze_sentence}'\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüìå Total words: {analysis['input_words']}\")\n",
        "print(f\"üìå Words found in document: {len(analysis['matching_words'])}\")\n",
        "print(f\"üìå Match ratio: {analysis['match_ratio']:.1%}\")\n",
        "print(f\"\\nüìå Matching words: {', '.join(analysis['matching_words'][:15])}\")\n",
        "\n",
        "print(\"\\nüìå Similar sentences from document:\")\n",
        "print(\"-\"*60)\n",
        "for i, (sent, score) in enumerate(analysis['similar_sentences'], 1):\n",
        "    print(f\"\\n{i}. [Score: {score:.3f}]\")\n",
        "    print(f\"   {sent[:100]}{'...' if len(sent) > 100 else ''}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Quick Expansion Function\n",
        "\n",
        "Use this for quick expansions without the form interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def expand_sentence(sentence, method='hybrid', num_sentences=4):\n",
        "    \"\"\"\n",
        "    Quick function to expand a sentence.\n",
        "    \n",
        "    Args:\n",
        "        sentence: Your input sentence\n",
        "        method: 'hybrid', 'markov', or 'similarity'\n",
        "        num_sentences: Number of sentences in output (2-8)\n",
        "    \"\"\"\n",
        "    result = expander.expand(sentence, method=method, num_sentences=num_sentences)\n",
        "    print(f\"\\nüìù Input: {sentence}\")\n",
        "    print(f\"\\nüìÑ Output ({num_sentences} sentences, {method} method):\")\n",
        "    print(\"-\"*60)\n",
        "    print(textwrap.fill(result, width=70))\n",
        "    print(\"-\"*60)\n",
        "    return result\n",
        "\n",
        "# Example usage:\n",
        "# expand_sentence(\"The night was dark and cold\")\n",
        "# expand_sentence(\"She opened the ancient book\", method='similarity', num_sentences=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Try It Out!\n",
        "\n",
        "Modify the sentences below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try your own sentences here!\n",
        "\n",
        "# Example 1 - Hybrid method (default, best results)\n",
        "expand_sentence(\"The journey had just begun\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# Example 2 - Using Markov Chain only\n",
        "expand_sentence(\"He walked through the darkness\", method='markov')\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# Example 3 - Using Similarity only\n",
        "expand_sentence(\"The secret was finally revealed\", method='similarity', num_sentences=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìö Tips for Best Results\n",
        "\n",
        "1. **Use vocabulary from your document** - The system works best when your input contains words that appear in the source document.\n",
        "\n",
        "2. **Try different methods:**\n",
        "   - `hybrid` - Best overall results\n",
        "   - `markov` - More creative/random output\n",
        "   - `similarity` - Closest to original document style\n",
        "\n",
        "3. **Longer documents = better results** - More text provides more patterns to learn.\n",
        "\n",
        "4. **Analyze first** - Use the analysis tool to check if your input words exist in the document.\n",
        "\n",
        "---\n",
        "\n",
        "**No AI/LLM used!** This system uses statistical methods (Markov Chain + TF-IDF) only."
      ]
    }
  ]
}
