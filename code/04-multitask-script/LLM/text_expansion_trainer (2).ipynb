{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvjr3IevclFC"
      },
      "source": [
        "# ğŸ“– Text Expansion AI - Training on Novel Chapters\n",
        "\n",
        "This notebook will:\n",
        "1. Load a lightweight AI model (GPT-2 Small)\n",
        "2. Fine-tune it on your markdown novel chapters\n",
        "3. Generate expanded paragraphs from single sentence inputs\n",
        "\n",
        "**No authentication required!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXLU9AibclFE"
      },
      "source": [
        "## 1ï¸âƒ£ Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC_XmrdMclFG",
        "outputId": "52221130-54a2-4643-b354-5a12fe4d6ede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m145.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m841.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m142.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.6 requires torch<2.10,>=1.10, but you have torch 2.10.0 which is incompatible.\n",
            "cuda-python 12.9.5 requires cuda-bindings~=12.9.5, but you have cuda-bindings 12.9.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ… Installation complete!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers peft accelerate datasets bitsandbytes\n",
        "!pip install -q torch torchvision torchaudio --upgrade\n",
        "\n",
        "print(\"âœ… Installation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7t0B2sQclFH"
      },
      "source": [
        "## 2ï¸âƒ£ Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew0Xr5vQclFH",
        "outputId": "771c355a-151c-4492-84dc-9a56a12dcd35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Memory: 14.7 GB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TextGenerationPipeline\n",
        ")\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJe3WCoCclFI"
      },
      "source": [
        "## 3ï¸âƒ£ Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7L27gvHclFI",
        "outputId": "6088df56-7a3c-4db0-b163-a7b091dff52b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration loaded!\n",
            "Model: gpt2\n",
            "Epochs: 3\n",
            "Batch Size: 4\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CONFIGURATION - Modify these as needed\n",
        "# ============================================\n",
        "\n",
        "# Model selection (all are free and don't require auth)\n",
        "# Options: \"gpt2\", \"gpt2-medium\", \"distilgpt2\", \"facebook/opt-125m\"\n",
        "MODEL_NAME = \"gpt2\"  # Smallest and fastest for training\n",
        "\n",
        "# Training parameters\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 4  # Reduce if out of memory\n",
        "LEARNING_RATE = 2e-4\n",
        "MAX_LENGTH = 512  # Maximum sequence length\n",
        "\n",
        "# LoRA parameters (for efficient fine-tuning)\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.1\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = \"./text_expansion_model\"\n",
        "\n",
        "print(\"Configuration loaded!\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Kt7d6iAclFJ"
      },
      "source": [
        "## 4ï¸âƒ£ Upload Your Markdown Files\n",
        "\n",
        "Upload your markdown files containing novel chapters. The files should contain English text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J_fYnpxclFJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Create directory for uploads\n",
        "os.makedirs(\"./markdown_data\", exist_ok=True)\n",
        "\n",
        "print(\"ğŸ“ Please upload your markdown (.md) files:\")\n",
        "print(\"(You can select multiple files at once)\")\n",
        "print()\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Save uploaded files\n",
        "for filename, content in uploaded.items():\n",
        "    filepath = f\"./markdown_data/{filename}\"\n",
        "    with open(filepath, 'wb') as f:\n",
        "        f.write(content)\n",
        "    print(f\"âœ… Saved: {filename}\")\n",
        "\n",
        "print(f\"\\nTotal files uploaded: {len(uploaded)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbkeRPyUclFJ"
      },
      "source": [
        "### Alternative: Use Sample Data (if no files to upload)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pY0tX8hclFK"
      },
      "outputs": [],
      "source": [
        "# Run this cell ONLY if you want to use sample data instead of uploading files\n",
        "\n",
        "SAMPLE_DATA = \"\"\"\n",
        "# Chapter 1: The Beginning\n",
        "\n",
        "The morning sun cast long shadows across the ancient city. Sarah stood at the edge of the marketplace, her eyes scanning the crowd for any sign of her contact. The air was thick with the scent of spices and the sound of merchants calling out their wares.\n",
        "\n",
        "She had traveled far to reach this place, crossing mountains and rivers that few had dared to traverse. Her journey had begun three months ago, when a mysterious letter arrived at her doorstep. The letter spoke of an ancient artifact, hidden deep within the ruins of a forgotten temple.\n",
        "\n",
        "# Chapter 2: The Discovery\n",
        "\n",
        "Deep within the temple, Sarah discovered something that would change everything. The artifact glowed with an ethereal light, pulsing like a heartbeat in the darkness. She reached out tentatively, her fingers trembling as they approached the smooth surface.\n",
        "\n",
        "The moment her skin made contact, visions flooded her mind. She saw civilizations rise and fall, witnessed the birth of stars and the death of worlds. Knowledge that had been lost for millennia now coursed through her consciousness.\n",
        "\n",
        "# Chapter 3: The Journey Home\n",
        "\n",
        "The return journey proved even more treacherous than the voyage out. Sarah clutched the artifact close to her chest, aware that countless dangers lurked in every shadow. Those who sought the artifact's power would stop at nothing to claim it.\n",
        "\n",
        "As she walked through the moonlit forest, she could hear whispers in the darkness. The trees seemed to bend toward her, as if they too sensed the power she carried. She quickened her pace, knowing that safety was still many miles away.\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"./markdown_data\", exist_ok=True)\n",
        "with open(\"./markdown_data/sample_novel.md\", \"w\") as f:\n",
        "    f.write(SAMPLE_DATA)\n",
        "\n",
        "print(\"âœ… Sample data created: ./markdown_data/sample_novel.md\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmuMAOl7clFK"
      },
      "source": [
        "## 5ï¸âƒ£ Process Markdown Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGhPo04jclFK"
      },
      "outputs": [],
      "source": [
        "def clean_markdown(text):\n",
        "    \"\"\"Clean markdown formatting and extract pure text.\"\"\"\n",
        "    # Remove markdown headers but keep the text\n",
        "    text = re.sub(r'^#{1,6}\\s*', '', text, flags=re.MULTILINE)\n",
        "    # Remove bold/italic markers\n",
        "    text = re.sub(r'\\*{1,2}([^*]+)\\*{1,2}', r'\\1', text)\n",
        "    text = re.sub(r'_{1,2}([^_]+)_{1,2}', r'\\1', text)\n",
        "    # Remove links but keep text\n",
        "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)\n",
        "    # Remove images\n",
        "    text = re.sub(r'!\\[([^\\]]*)\\]\\([^)]+\\)', '', text)\n",
        "    # Remove code blocks\n",
        "    text = re.sub(r'```[^`]*```', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'`[^`]+`', '', text)\n",
        "    # Clean up whitespace\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "def extract_paragraphs(text):\n",
        "    \"\"\"Extract paragraphs from text.\"\"\"\n",
        "    paragraphs = []\n",
        "    for para in text.split('\\n\\n'):\n",
        "        para = para.strip()\n",
        "        # Only keep substantial paragraphs\n",
        "        if len(para) > 50 and len(para.split()) > 10:\n",
        "            paragraphs.append(para)\n",
        "    return paragraphs\n",
        "\n",
        "def create_training_pairs(paragraphs):\n",
        "    \"\"\"\n",
        "    Create training pairs: first sentence -> full paragraph.\n",
        "    This teaches the model to expand sentences into paragraphs.\n",
        "    \"\"\"\n",
        "    training_data = []\n",
        "\n",
        "    for para in paragraphs:\n",
        "        # Extract first sentence\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
        "        if len(sentences) >= 2:  # Need at least 2 sentences\n",
        "            first_sentence = sentences[0].strip()\n",
        "            # Create training format\n",
        "            training_text = f\"<|input|>{first_sentence}<|expand|>{para}<|end|>\"\n",
        "            training_data.append({\"text\": training_text})\n",
        "\n",
        "    return training_data\n",
        "\n",
        "# Process all markdown files\n",
        "all_paragraphs = []\n",
        "markdown_dir = Path(\"./markdown_data\")\n",
        "\n",
        "for md_file in markdown_dir.glob(\"*.md\"):\n",
        "    print(f\"Processing: {md_file.name}\")\n",
        "    with open(md_file, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    cleaned = clean_markdown(content)\n",
        "    paragraphs = extract_paragraphs(cleaned)\n",
        "    all_paragraphs.extend(paragraphs)\n",
        "    print(f\"  Found {len(paragraphs)} paragraphs\")\n",
        "\n",
        "print(f\"\\nTotal paragraphs collected: {len(all_paragraphs)}\")\n",
        "\n",
        "# Create training pairs\n",
        "training_data = create_training_pairs(all_paragraphs)\n",
        "print(f\"Training pairs created: {len(training_data)}\")\n",
        "\n",
        "# Preview a sample\n",
        "if training_data:\n",
        "    print(\"\\nğŸ“ Sample training data:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(training_data[0][\"text\"][:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf4knnU9clFL"
      },
      "source": [
        "## 6ï¸âƒ£ Load Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBuIrZZLclFL"
      },
      "outputs": [],
      "source": [
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Add special tokens\n",
        "special_tokens = {\n",
        "    \"pad_token\": \"<|pad|>\",\n",
        "    \"additional_special_tokens\": [\"<|input|>\", \"<|expand|>\", \"<|end|>\"]\n",
        "}\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None\n",
        ")\n",
        "\n",
        "# Resize token embeddings for new special tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(f\"\\nâœ… Model loaded successfully!\")\n",
        "print(f\"Parameters: {model.num_parameters():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1T5uoG7clFL"
      },
      "source": [
        "## 7ï¸âƒ£ Setup LoRA for Efficient Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvZ7VKvsclFL"
      },
      "outputs": [],
      "source": [
        "# Configure LoRA (Low-Rank Adaptation)\n",
        "# This allows efficient fine-tuning with much less memory\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 attention layers\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"\\nâœ… LoRA applied!\")\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "print(f\"Total parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cALYvSS5clFM"
      },
      "source": [
        "## 8ï¸âƒ£ Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCx06TuAclFM"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the training data.\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "# Create dataset\n",
        "dataset = Dataset.from_list(training_data)\n",
        "\n",
        "# Tokenize dataset\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "# Split into train and validation\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "print(f\"\\nâœ… Dataset prepared!\")\n",
        "print(f\"Training samples: {len(split_dataset['train'])}\")\n",
        "print(f\"Validation samples: {len(split_dataset['test'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYZ9vepzclFM"
      },
      "source": [
        "## 9ï¸âƒ£ Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8kYdyaMclFM"
      },
      "outputs": [],
      "source": [
        "# Data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # We're doing causal LM, not masked LM\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    warmup_steps=100,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=device == \"cuda\",\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",  # Disable wandb\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 16\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=split_dataset[\"train\"],\n",
        "    eval_dataset=split_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"ğŸš€ Starting training...\")\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q92sTPj5clFN"
      },
      "outputs": [],
      "source": [
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nâœ… Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHbLndFGclFN"
      },
      "source": [
        "## ğŸ”Ÿ Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rXzRE1FclFN",
        "outputId": "af8ad80a-52ec-4835-ebe0-ca42b59d7240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model saved to: ./text_expansion_model\n"
          ]
        }
      ],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"âœ… Model saved to: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOj1czRlclFN"
      },
      "source": [
        "---\n",
        "\n",
        "# ğŸ¯ Text Expansion - Use Your Trained Model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcyzEtjUclFN",
        "outputId": "20564c65-64c6-41dd-ae10-0aecfbdfffdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Text expansion function ready!\n"
          ]
        }
      ],
      "source": [
        "def expand_text(input_sentence, max_new_tokens=200, temperature=0.8, top_p=0.9, num_return_sequences=1):\n",
        "    \"\"\"\n",
        "    Expand a single sentence into a paragraph.\n",
        "\n",
        "    Args:\n",
        "        input_sentence: The sentence to expand\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "        temperature: Creativity level (0.1-1.0, higher = more creative)\n",
        "        top_p: Nucleus sampling parameter\n",
        "        num_return_sequences: Number of different expansions to generate\n",
        "\n",
        "    Returns:\n",
        "        List of expanded paragraphs\n",
        "    \"\"\"\n",
        "    # Format input\n",
        "    prompt = f\"<|input|>{input_sentence}<|expand|>\"\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    if device == \"cuda\":\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.convert_tokens_to_ids(\"<|end|>\"),\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "    # Decode and extract expanded text\n",
        "    results = []\n",
        "    for output in outputs:\n",
        "        text = tokenizer.decode(output, skip_special_tokens=False)\n",
        "        # Extract the expanded part\n",
        "        if \"<|expand|>\" in text:\n",
        "            expanded = text.split(\"<|expand|>\")[1]\n",
        "            if \"<|end|>\" in expanded:\n",
        "                expanded = expanded.split(\"<|end|>\")[0]\n",
        "            # Clean up special tokens\n",
        "            expanded = expanded.replace(\"<|pad|>\", \"\").strip()\n",
        "            results.append(expanded)\n",
        "        else:\n",
        "            results.append(text)\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"âœ… Text expansion function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSbSz6hyclFO"
      },
      "source": [
        "## ğŸ“ Try It Out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY93R04EclFO"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "test_sentences = [\n",
        "    \"The old mansion stood alone on the hill.\",\n",
        "    \"She opened the letter with trembling hands.\",\n",
        "    \"The storm was approaching fast.\"\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TEXT EXPANSION EXAMPLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    print(f\"\\nğŸ“¥ INPUT: {sentence}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    expanded = expand_text(sentence, max_new_tokens=150)\n",
        "\n",
        "    print(f\"ğŸ“¤ OUTPUT:\")\n",
        "    print(expanded[0])\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H80jfxgPclFO"
      },
      "source": [
        "## ğŸ® Interactive Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCHsJ5HHclFO"
      },
      "outputs": [],
      "source": [
        "# Interactive text expansion\n",
        "print(\"=\"*60)\n",
        "print(\"INTERACTIVE TEXT EXPANSION\")\n",
        "print(\"=\"*60)\n",
        "print(\"Enter a sentence and get an expanded paragraph!\")\n",
        "print(\"Type 'quit' to exit.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"ğŸ“ Enter your sentence: \")\n",
        "\n",
        "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "        print(\"Goodbye! ğŸ‘‹\")\n",
        "        break\n",
        "\n",
        "    if not user_input.strip():\n",
        "        print(\"Please enter a valid sentence.\")\n",
        "        continue\n",
        "\n",
        "    print(\"\\nâ³ Generating...\\n\")\n",
        "    expanded = expand_text(user_input, max_new_tokens=200)\n",
        "\n",
        "    print(\"ğŸ“– EXPANDED PARAGRAPH:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(expanded[0])\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiMYmytlclFO"
      },
      "source": [
        "## ğŸ“¦ Download the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mOQ6kwzclFO"
      },
      "outputs": [],
      "source": [
        "# Zip and download the model\n",
        "import shutil\n",
        "\n",
        "# Create zip file\n",
        "shutil.make_archive(\"text_expansion_model\", 'zip', OUTPUT_DIR)\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "files.download(\"text_expansion_model.zip\")\n",
        "\n",
        "print(\"âœ… Model downloaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUOaluP-clFP"
      },
      "source": [
        "---\n",
        "\n",
        "# ğŸ“š Load Previously Trained Model\n",
        "\n",
        "Use this section if you want to load a previously trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38tPIP11clFP"
      },
      "outputs": [],
      "source": [
        "# Upload previously trained model (zip file)\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "print(\"Upload your trained model (zip file):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall('./loaded_model')\n",
        "        print(f\"âœ… Model extracted to: ./loaded_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQxb94u0clFP"
      },
      "outputs": [],
      "source": [
        "# Load the saved model\n",
        "from peft import PeftModel\n",
        "\n",
        "LOADED_MODEL_PATH = \"./loaded_model\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(LOADED_MODEL_PATH)\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None\n",
        ")\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Load LoRA weights\n",
        "model = PeftModel.from_pretrained(base_model, LOADED_MODEL_PATH)\n",
        "\n",
        "print(\"âœ… Model loaded! Ready to use.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAWQUMStclFQ"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸ’¡ Tips for Better Results\n",
        "\n",
        "1. **More training data**: The more novel chapters you provide, the better the model will learn your writing style.\n",
        "\n",
        "2. **Adjust temperature**:\n",
        "   - Lower (0.3-0.5): More focused, predictable output\n",
        "   - Higher (0.7-1.0): More creative, diverse output\n",
        "\n",
        "3. **Training epochs**:\n",
        "   - 3-5 epochs is usually enough\n",
        "   - Too many epochs may cause overfitting\n",
        "\n",
        "4. **Input sentences**:\n",
        "   - Start with descriptive sentences\n",
        "   - Include setting, character, or action details"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}