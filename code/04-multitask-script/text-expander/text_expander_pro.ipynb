{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJqs-pHUVm7c"
      },
      "source": [
        "# ðŸ“ Text Expander Pro - AI-Enhanced Edition\n",
        "\n",
        "**Expand sentences into paragraphs using multiple AI techniques.**\n",
        "\n",
        "This notebook offers **4 levels** of text generation:\n",
        "\n",
        "| Level | Method | Description |\n",
        "|-------|--------|-------------|\n",
        "| ðŸŸ¢ Basic | Markov Chain | Statistical word transitions |\n",
        "| ðŸŸ¡ Enhanced | Word2Vec + Markov | Semantic word relationships |\n",
        "| ðŸŸ  Advanced | LSTM Neural Network | Custom trained on your document |\n",
        "| ðŸ”´ Pro | Fine-tuned GPT-2 | State-of-the-art language model |\n",
        "\n",
        "---\n",
        "\n",
        "## âš¡ Quick Start\n",
        "1. **Enable GPU**: Runtime â†’ Change runtime type â†’ T4 GPU (recommended for LSTM/GPT-2)\n",
        "2. Run cells in order\n",
        "3. Upload your document\n",
        "4. Choose your method and expand!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGNd-PhcVm7e"
      },
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n",
        "This installs the required libraries for all AI methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvprn5HKVm7f",
        "outputId": "a4717b13-ddbb-4632-d679-243984fe84a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… All packages installed!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q gensim # Update gensim to the latest compatible version\n",
        "!pip install -q sentence-transformers  # Better similarity\n",
        "!pip install -q transformers accelerate  # GPT-2\n",
        "!pip install -q torch  # PyTorch for neural networks\n",
        "\n",
        "print(\"âœ… All packages installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd7ae7e3",
        "outputId": "a29b7551-be13-467e-cb66-89f36dfbbc57"
      },
      "source": [
        "# Import all libraries\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "import textwrap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Gensim for Word2Vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Sentence Transformers for better similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# PyTorch for LSTM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Transformers for GPT-2\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"âœ… Libraries imported!\")\n",
        "print(f\"ðŸ–¥ï¸  Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ðŸŽ® GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Libraries imported!\n",
            "ðŸ–¥ï¸  Device: cuda\n",
            "ðŸŽ® GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-Di4CoCVm7h"
      },
      "source": [
        "## Step 2: Define All Model Classes\n",
        "\n",
        "This cell contains all the AI models and text processing logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH3GnHlNVm7i",
        "outputId": "9aa02b2f-b3fe-48cf-a4dd-8c23bec547e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DocumentProcessor defined!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# DOCUMENT PROCESSOR\n",
        "# =============================================================================\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Process and clean markdown documents\"\"\"\n",
        "\n",
        "    def __init__(self, text: str = None):\n",
        "        self.raw_text = text if text else \"\"\n",
        "        self.sentences = []\n",
        "        self.words = []\n",
        "        self.paragraphs = []\n",
        "        self.tokenized_sentences = []  # For Word2Vec training\n",
        "\n",
        "    def clean_markdown(self, text: str) -> str:\n",
        "        \"\"\"Remove markdown syntax\"\"\"\n",
        "        text = re.sub(r'^#{1,6}\\s+', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'\\*{1,3}(.*?)\\*{1,3}', r'\\1', text)\n",
        "        text = re.sub(r'_{1,3}(.*?)_{1,3}', r'\\1', text)\n",
        "        text = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', text)\n",
        "        text = re.sub(r'!\\[([^\\]]*)\\]\\([^\\)]+\\)', '', text)\n",
        "        text = re.sub(r'```[\\s\\S]*?```', '', text)\n",
        "        text = re.sub(r'`([^`]+)`', r'\\1', text)\n",
        "        text = re.sub(r'^[-*_]{3,}\\s*$', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'^>\\s+', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'^[\\s]*[-*+]\\s+', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'^[\\s]*\\d+\\.\\s+', '', text, flags=re.MULTILINE)\n",
        "        return text\n",
        "\n",
        "    def extract_sentences(self, text: str) -> list:\n",
        "        \"\"\"Extract sentences from text\"\"\"\n",
        "        # Handle abbreviations\n",
        "        abbrevs = ['Mr', 'Mrs', 'Ms', 'Dr', 'Prof', 'St', 'Jr', 'Sr']\n",
        "        for abbr in abbrevs:\n",
        "            text = re.sub(rf'{abbr}\\.', abbr, text)\n",
        "\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 15]\n",
        "        return sentences\n",
        "\n",
        "    def extract_words(self, text: str) -> list:\n",
        "        \"\"\"Extract words from text\"\"\"\n",
        "        words = re.findall(r\"\\b[a-zA-Z]+(?:'[a-zA-Z]+)?\\b\", text.lower())\n",
        "        return words\n",
        "\n",
        "    def extract_paragraphs(self, text: str) -> list:\n",
        "        \"\"\"Extract paragraphs from text\"\"\"\n",
        "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "        paragraphs = [p.strip() for p in paragraphs if p.strip() and len(p.strip()) > 50]\n",
        "        return paragraphs\n",
        "\n",
        "    def tokenize_for_training(self, sentences: list) -> list:\n",
        "        \"\"\"Tokenize sentences for Word2Vec training\"\"\"\n",
        "        tokenized = []\n",
        "        for sent in sentences:\n",
        "            words = re.findall(r\"\\b[a-zA-Z]+(?:'[a-zA-Z]+)?\\b\", sent.lower())\n",
        "            if len(words) > 2:\n",
        "                tokenized.append(words)\n",
        "        return tokenized\n",
        "\n",
        "    def process(self) -> dict:\n",
        "        \"\"\"Process the complete document\"\"\"\n",
        "        cleaned = self.clean_markdown(self.raw_text)\n",
        "\n",
        "        self.sentences = self.extract_sentences(cleaned)\n",
        "        self.words = self.extract_words(cleaned)\n",
        "        self.paragraphs = self.extract_paragraphs(cleaned)\n",
        "        self.tokenized_sentences = self.tokenize_for_training(self.sentences)\n",
        "\n",
        "        return {\n",
        "            'sentences': self.sentences,\n",
        "            'words': self.words,\n",
        "            'paragraphs': self.paragraphs,\n",
        "            'tokenized': self.tokenized_sentences,\n",
        "            'word_count': len(self.words),\n",
        "            'sentence_count': len(self.sentences),\n",
        "            'unique_words': len(set(self.words)),\n",
        "            'cleaned_text': cleaned\n",
        "        }\n",
        "\n",
        "print(\"âœ… DocumentProcessor defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP_y8yIKVm7i",
        "outputId": "503acc12-c5fa-4be3-a112-9f84ef6d53f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… MarkovChain defined!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ðŸŸ¢ BASIC: MARKOV CHAIN\n",
        "# =============================================================================\n",
        "\n",
        "class MarkovChain:\n",
        "    \"\"\"Basic Markov Chain for text generation\"\"\"\n",
        "\n",
        "    def __init__(self, order: int = 2):\n",
        "        self.order = order\n",
        "        self.chain = defaultdict(list)\n",
        "        self.starters = []\n",
        "\n",
        "    def train(self, sentences: list):\n",
        "        \"\"\"Train the model from sentences\"\"\"\n",
        "        for sentence in sentences:\n",
        "            words = sentence.split()\n",
        "            if len(words) < self.order + 1:\n",
        "                continue\n",
        "\n",
        "            starter = tuple(words[:self.order])\n",
        "            self.starters.append(starter)\n",
        "\n",
        "            for i in range(len(words) - self.order):\n",
        "                key = tuple(words[i:i + self.order])\n",
        "                next_word = words[i + self.order]\n",
        "                self.chain[key].append(next_word)\n",
        "\n",
        "    def generate(self, seed_words: list = None, max_words: int = 50) -> str:\n",
        "        \"\"\"Generate text\"\"\"\n",
        "        if not self.starters:\n",
        "            return \"\"\n",
        "\n",
        "        if seed_words and len(seed_words) >= self.order:\n",
        "            current = self._find_matching_key(seed_words)\n",
        "        else:\n",
        "            current = random.choice(self.starters)\n",
        "\n",
        "        if not current:\n",
        "            current = random.choice(self.starters)\n",
        "\n",
        "        result = list(current)\n",
        "\n",
        "        for _ in range(max_words - self.order):\n",
        "            if current not in self.chain:\n",
        "                current = self._find_similar_key(current)\n",
        "                if not current:\n",
        "                    break\n",
        "\n",
        "            next_words = self.chain.get(current, [])\n",
        "            if not next_words:\n",
        "                break\n",
        "\n",
        "            next_word = random.choice(next_words)\n",
        "            result.append(next_word)\n",
        "            current = tuple(result[-self.order:])\n",
        "\n",
        "            if next_word.endswith(('.', '!', '?')):\n",
        "                break\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "    def _find_matching_key(self, words: list) -> tuple:\n",
        "        words_lower = [w.lower() for w in words]\n",
        "        for i in range(len(words_lower) - self.order + 1):\n",
        "            key = tuple(words_lower[i:i + self.order])\n",
        "            if key in self.chain:\n",
        "                return key\n",
        "        for key in self.chain.keys():\n",
        "            key_lower = tuple(w.lower() for w in key)\n",
        "            if any(w in key_lower for w in words_lower):\n",
        "                return key\n",
        "        return random.choice(self.starters) if self.starters else None\n",
        "\n",
        "    def _find_similar_key(self, current: tuple) -> tuple:\n",
        "        current_lower = tuple(w.lower() for w in current)\n",
        "        for key in self.chain.keys():\n",
        "            key_lower = tuple(w.lower() for w in key)\n",
        "            if any(w in current_lower for w in key_lower):\n",
        "                return key\n",
        "        return random.choice(self.starters) if self.starters else None\n",
        "\n",
        "print(\"âœ… MarkovChain defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ8nc1R-Vm7j",
        "outputId": "546443aa-0d00-4f5d-c57f-70ed3040b7c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Word2VecMarkov defined!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ðŸŸ¡ ENHANCED: WORD2VEC + MARKOV\n",
        "# =============================================================================\n",
        "\n",
        "class Word2VecMarkov:\n",
        "    \"\"\"Enhanced Markov Chain using Word2Vec for semantic word selection\"\"\"\n",
        "\n",
        "    def __init__(self, order: int = 2, vector_size: int = 100, window: int = 5):\n",
        "        self.order = order\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.chain = defaultdict(list)\n",
        "        self.starters = []\n",
        "        self.word2vec = None\n",
        "        self.vocab = set()\n",
        "\n",
        "    def train(self, sentences: list, tokenized_sentences: list):\n",
        "        \"\"\"Train both Markov Chain and Word2Vec\"\"\"\n",
        "        # Train Markov Chain\n",
        "        for sentence in sentences:\n",
        "            words = sentence.split()\n",
        "            if len(words) < self.order + 1:\n",
        "                continue\n",
        "            starter = tuple(words[:self.order])\n",
        "            self.starters.append(starter)\n",
        "            for i in range(len(words) - self.order):\n",
        "                key = tuple(words[i:i + self.order])\n",
        "                next_word = words[i + self.order]\n",
        "                self.chain[key].append(next_word)\n",
        "\n",
        "        # Train Word2Vec\n",
        "        print(\"   Training Word2Vec model...\")\n",
        "        self.word2vec = Word2Vec(\n",
        "            sentences=tokenized_sentences,\n",
        "            vector_size=self.vector_size,\n",
        "            window=self.window,\n",
        "            min_count=1,\n",
        "            workers=4,\n",
        "            epochs=50\n",
        "        )\n",
        "        self.vocab = set(self.word2vec.wv.key_to_index.keys())\n",
        "        print(f\"   Word2Vec vocabulary: {len(self.vocab)} words\")\n",
        "\n",
        "    def _get_best_next_word(self, candidates: list, context: list) -> str:\n",
        "        \"\"\"Select the best next word using Word2Vec similarity\"\"\"\n",
        "        if not candidates or not self.word2vec:\n",
        "            return random.choice(candidates) if candidates else \"\"\n",
        "\n",
        "        # Get context words that are in vocabulary\n",
        "        context_words = [w.lower() for w in context if w.lower() in self.vocab]\n",
        "\n",
        "        if not context_words:\n",
        "            return random.choice(candidates)\n",
        "\n",
        "        # Score each candidate based on similarity to context\n",
        "        scored = []\n",
        "        for candidate in candidates:\n",
        "            cand_lower = candidate.lower().rstrip('.,!?\"\\'')\n",
        "            if cand_lower in self.vocab:\n",
        "                # Calculate average similarity to context words\n",
        "                similarities = []\n",
        "                for ctx_word in context_words[-5:]:  # Use last 5 context words\n",
        "                    try:\n",
        "                        sim = self.word2vec.wv.similarity(cand_lower, ctx_word)\n",
        "                        similarities.append(sim)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                if similarities:\n",
        "                    avg_sim = sum(similarities) / len(similarities)\n",
        "                    # Add some randomness to avoid repetition\n",
        "                    score = avg_sim + random.uniform(-0.1, 0.1)\n",
        "                    scored.append((candidate, score))\n",
        "                else:\n",
        "                    scored.append((candidate, random.uniform(-0.5, 0.5)))\n",
        "            else:\n",
        "                scored.append((candidate, random.uniform(-0.5, 0.5)))\n",
        "\n",
        "        # Sort by score and pick from top candidates with some randomness\n",
        "        scored.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_n = min(3, len(scored))\n",
        "        return random.choice([s[0] for s in scored[:top_n]])\n",
        "\n",
        "    def generate(self, seed_words: list = None, max_words: int = 50) -> str:\n",
        "        \"\"\"Generate text using Word2Vec-enhanced selection\"\"\"\n",
        "        if not self.starters:\n",
        "            return \"\"\n",
        "\n",
        "        if seed_words and len(seed_words) >= self.order:\n",
        "            current = self._find_matching_key(seed_words)\n",
        "        else:\n",
        "            current = random.choice(self.starters)\n",
        "\n",
        "        if not current:\n",
        "            current = random.choice(self.starters)\n",
        "\n",
        "        result = list(current)\n",
        "\n",
        "        for _ in range(max_words - self.order):\n",
        "            if current not in self.chain:\n",
        "                current = self._find_similar_key(current)\n",
        "                if not current:\n",
        "                    break\n",
        "\n",
        "            candidates = self.chain.get(current, [])\n",
        "            if not candidates:\n",
        "                break\n",
        "\n",
        "            # Use Word2Vec to select best next word\n",
        "            next_word = self._get_best_next_word(candidates, result)\n",
        "            result.append(next_word)\n",
        "            current = tuple(result[-self.order:])\n",
        "\n",
        "            if next_word.endswith(('.', '!', '?')):\n",
        "                break\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "    def _find_matching_key(self, words: list) -> tuple:\n",
        "        words_lower = [w.lower() for w in words]\n",
        "        for i in range(len(words_lower) - self.order + 1):\n",
        "            key = tuple(words_lower[i:i + self.order])\n",
        "            if key in self.chain:\n",
        "                return key\n",
        "        for key in self.chain.keys():\n",
        "            key_lower = tuple(w.lower() for w in key)\n",
        "            if any(w in key_lower for w in words_lower):\n",
        "                return key\n",
        "        return None\n",
        "\n",
        "    def _find_similar_key(self, current: tuple) -> tuple:\n",
        "        \"\"\"Find similar key using Word2Vec\"\"\"\n",
        "        current_lower = [w.lower() for w in current]\n",
        "\n",
        "        best_key = None\n",
        "        best_score = -1\n",
        "\n",
        "        for key in self.chain.keys():\n",
        "            key_lower = [w.lower() for w in key]\n",
        "\n",
        "            # Calculate similarity between keys\n",
        "            score = 0\n",
        "            count = 0\n",
        "            for w1 in current_lower:\n",
        "                for w2 in key_lower:\n",
        "                    if w1 in self.vocab and w2 in self.vocab:\n",
        "                        try:\n",
        "                            score += self.word2vec.wv.similarity(w1, w2)\n",
        "                            count += 1\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "            if count > 0:\n",
        "                avg_score = score / count\n",
        "                if avg_score > best_score:\n",
        "                    best_score = avg_score\n",
        "                    best_key = key\n",
        "\n",
        "        return best_key if best_key else (random.choice(self.starters) if self.starters else None)\n",
        "\n",
        "print(\"âœ… Word2VecMarkov defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAQPlvJ2Vm7l",
        "outputId": "6efd3217-648a-485a-9353-d136b9111769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… LSTMTextGenerator defined!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ðŸŸ  ADVANCED: LSTM LANGUAGE MODEL\n",
        "# =============================================================================\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    \"\"\"Character-level LSTM for text generation\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size=128, hidden_size=256, num_layers=2):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embed = self.embedding(x)\n",
        "        output, hidden = self.lstm(embed, hidden)\n",
        "        output = self.fc(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        return (h0, c0)\n",
        "\n",
        "\n",
        "class LSTMTextGenerator:\n",
        "    \"\"\"LSTM-based text generator trained on your document\"\"\"\n",
        "\n",
        "    def __init__(self, embed_size=128, hidden_size=256, num_layers=2):\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.model = None\n",
        "        self.char_to_idx = {}\n",
        "        self.idx_to_char = {}\n",
        "        self.vocab_size = 0\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.trained = False\n",
        "\n",
        "    def train(self, text: str, epochs: int = 20, seq_length: int = 100, batch_size: int = 64, lr: float = 0.002):\n",
        "        \"\"\"Train the LSTM model on the text\"\"\"\n",
        "        print(f\"   Preparing data...\")\n",
        "\n",
        "        # Build vocabulary\n",
        "        chars = sorted(list(set(text)))\n",
        "        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "        self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "        self.vocab_size = len(chars)\n",
        "\n",
        "        print(f\"   Vocabulary size: {self.vocab_size} characters\")\n",
        "\n",
        "        # Create sequences\n",
        "        encoded = [self.char_to_idx[ch] for ch in text]\n",
        "        sequences = []\n",
        "        targets = []\n",
        "\n",
        "        for i in range(0, len(encoded) - seq_length, seq_length // 2):\n",
        "            sequences.append(encoded[i:i + seq_length])\n",
        "            targets.append(encoded[i + 1:i + seq_length + 1])\n",
        "\n",
        "        X = torch.tensor(sequences, dtype=torch.long)\n",
        "        y = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(X, y)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = CharLSTM(\n",
        "            self.vocab_size,\n",
        "            self.embed_size,\n",
        "            self.hidden_size,\n",
        "            self.num_layers\n",
        "        ).to(self.device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        # Training loop\n",
        "        print(f\"   Training LSTM model...\")\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for batch_x, batch_y in dataloader:\n",
        "                batch_x = batch_x.to(self.device)\n",
        "                batch_y = batch_y.to(self.device)\n",
        "\n",
        "                hidden = self.model.init_hidden(batch_x.size(0), self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                output, hidden = self.model(batch_x, hidden)\n",
        "\n",
        "                loss = criterion(output.view(-1, self.vocab_size), batch_y.view(-1))\n",
        "                loss.backward()\n",
        "\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                avg_loss = total_loss / len(dataloader)\n",
        "                print(f\"   Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        self.trained = True\n",
        "        print(f\"   âœ“ LSTM training complete!\")\n",
        "\n",
        "    def generate(self, seed_text: str, length: int = 200, temperature: float = 0.8) -> str:\n",
        "        \"\"\"Generate text from seed\"\"\"\n",
        "        if not self.trained or not self.model:\n",
        "            return \"Model not trained yet.\"\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # Encode seed text\n",
        "        seed_encoded = [self.char_to_idx.get(ch, 0) for ch in seed_text[-100:]]\n",
        "        input_seq = torch.tensor([seed_encoded], dtype=torch.long).to(self.device)\n",
        "\n",
        "        hidden = self.model.init_hidden(1, self.device)\n",
        "\n",
        "        # Generate\n",
        "        generated = seed_text\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(length):\n",
        "                output, hidden = self.model(input_seq, hidden)\n",
        "\n",
        "                # Apply temperature\n",
        "                probs = torch.softmax(output[0, -1] / temperature, dim=0)\n",
        "\n",
        "                # Sample from distribution\n",
        "                idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "                char = self.idx_to_char[idx]\n",
        "                generated += char\n",
        "\n",
        "                # Update input\n",
        "                input_seq = torch.tensor([[idx]], dtype=torch.long).to(self.device)\n",
        "\n",
        "                # Stop at sentence end\n",
        "                if char in '.!?' and len(generated) > len(seed_text) + 50:\n",
        "                    break\n",
        "\n",
        "        return generated\n",
        "\n",
        "print(\"âœ… LSTMTextGenerator defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPj9XvyZVm7n",
        "outputId": "4201fc28-302a-4e2d-80c2-2639e4963a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… GPT2TextGenerator defined!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ðŸ”´ PRO: FINE-TUNED GPT-2\n",
        "# =============================================================================\n",
        "\n",
        "class GPT2TextGenerator:\n",
        "    \"\"\"Fine-tuned GPT-2 for text generation\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'distilgpt2'):\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.trained = False\n",
        "\n",
        "    def train(self, text: str, epochs: int = 3, batch_size: int = 4):\n",
        "        \"\"\"Fine-tune GPT-2 on the text\"\"\"\n",
        "        print(f\"   Loading {self.model_name}...\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
        "\n",
        "        # Add padding token\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.model.config.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "        # Move to device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Prepare training data\n",
        "        print(f\"   Preparing training data...\")\n",
        "\n",
        "        # Split text into chunks\n",
        "        max_length = 512\n",
        "        encodings = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_tensors='pt',\n",
        "            padding=True\n",
        "        )\n",
        "\n",
        "        # Create dataset\n",
        "        class TextDataset(Dataset):\n",
        "            def __init__(self, encodings):\n",
        "                self.input_ids = encodings['input_ids']\n",
        "                self.attention_mask = encodings['attention_mask']\n",
        "\n",
        "            def __len__(self):\n",
        "                return len(self.input_ids)\n",
        "\n",
        "            def __getitem__(self, idx):\n",
        "                return {\n",
        "                    'input_ids': self.input_ids[idx],\n",
        "                    'attention_mask': self.attention_mask[idx],\n",
        "                    'labels': self.input_ids[idx]\n",
        "                }\n",
        "\n",
        "        dataset = TextDataset(encodings)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Training\n",
        "        print(f\"   Fine-tuning GPT-2...\")\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-5)\n",
        "\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for batch in dataloader:\n",
        "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "\n",
        "                outputs = self.model(**batch)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            avg_loss = total_loss / len(dataloader)\n",
        "            print(f\"   Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        self.trained = True\n",
        "        print(f\"   âœ“ GPT-2 fine-tuning complete!\")\n",
        "\n",
        "    def generate(self, seed_text: str, max_length: int = 150, temperature: float = 0.9,\n",
        "                 top_k: int = 50, top_p: float = 0.95) -> str:\n",
        "        \"\"\"Generate text from seed\"\"\"\n",
        "        if not self.trained or not self.model:\n",
        "            return \"Model not trained yet.\"\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        input_ids = self.tokenizer.encode(seed_text, return_tensors='pt').to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                top_k=top_k,\n",
        "                top_p=top_p,\n",
        "                do_sample=True,\n",
        "                num_return_sequences=1,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                no_repeat_ngram_size=3\n",
        "            )\n",
        "\n",
        "        generated = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        return generated\n",
        "\n",
        "print(\"âœ… GPT2TextGenerator defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmRKzQzaVm7n",
        "outputId": "39047287-515c-4956-ec58-3e4f54a1e907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… EnhancedSimilarityFinder defined!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# SIMILARITY FINDER (Enhanced with Sentence Transformers)\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedSimilarityFinder:\n",
        "    \"\"\"Find similar sentences using Sentence Transformers\"\"\"\n",
        "\n",
        "    def __init__(self, sentences: list, paragraphs: list, use_transformers: bool = True):\n",
        "        self.sentences = sentences\n",
        "        self.paragraphs = paragraphs\n",
        "        self.use_transformers = use_transformers\n",
        "        self.sentence_embeddings = None\n",
        "        self.paragraph_embeddings = None\n",
        "        self.model = None\n",
        "\n",
        "        if use_transformers:\n",
        "            print(\"   Loading Sentence Transformer model...\")\n",
        "            self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            print(\"   Encoding sentences...\")\n",
        "            self.sentence_embeddings = self.model.encode(sentences, show_progress_bar=False)\n",
        "            print(\"   Encoding paragraphs...\")\n",
        "            self.paragraph_embeddings = self.model.encode(paragraphs, show_progress_bar=False)\n",
        "            print(f\"   âœ“ Encoded {len(sentences)} sentences and {len(paragraphs)} paragraphs\")\n",
        "\n",
        "    def find_similar_sentences(self, query: str, top_n: int = 5) -> list:\n",
        "        \"\"\"Find sentences most similar to query\"\"\"\n",
        "        if self.use_transformers and self.model:\n",
        "            query_embedding = self.model.encode([query])[0]\n",
        "\n",
        "            similarities = []\n",
        "            for i, sent_emb in enumerate(self.sentence_embeddings):\n",
        "                sim = np.dot(query_embedding, sent_emb) / (\n",
        "                    np.linalg.norm(query_embedding) * np.linalg.norm(sent_emb)\n",
        "                )\n",
        "                similarities.append((self.sentences[i], float(sim)))\n",
        "\n",
        "            similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "            return similarities[:top_n]\n",
        "        else:\n",
        "            # Fallback to simple matching\n",
        "            query_words = set(query.lower().split())\n",
        "            similarities = []\n",
        "            for sent in self.sentences:\n",
        "                sent_words = set(sent.lower().split())\n",
        "                overlap = len(query_words & sent_words) / max(len(query_words), 1)\n",
        "                similarities.append((sent, overlap))\n",
        "            similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "            return similarities[:top_n]\n",
        "\n",
        "    def find_similar_paragraphs(self, query: str, top_n: int = 3) -> list:\n",
        "        \"\"\"Find paragraphs most similar to query\"\"\"\n",
        "        if self.use_transformers and self.model:\n",
        "            query_embedding = self.model.encode([query])[0]\n",
        "\n",
        "            similarities = []\n",
        "            for i, para_emb in enumerate(self.paragraph_embeddings):\n",
        "                sim = np.dot(query_embedding, para_emb) / (\n",
        "                    np.linalg.norm(query_embedding) * np.linalg.norm(para_emb)\n",
        "                )\n",
        "                similarities.append((self.paragraphs[i], float(sim)))\n",
        "\n",
        "            similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "            return similarities[:top_n]\n",
        "        else:\n",
        "            query_words = set(query.lower().split())\n",
        "            similarities = []\n",
        "            for para in self.paragraphs:\n",
        "                para_words = set(para.lower().split())\n",
        "                overlap = len(query_words & para_words) / max(len(query_words), 1)\n",
        "                similarities.append((para, overlap))\n",
        "            similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "            return similarities[:top_n]\n",
        "\n",
        "print(\"âœ… EnhancedSimilarityFinder defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cXgR0vtVm7o",
        "outputId": "88337fec-84a6-4432-c87f-3fd1324aa8c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "âœ… All classes defined successfully!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# MAIN TEXT EXPANDER PRO CLASS\n",
        "# =============================================================================\n",
        "\n",
        "class TextExpanderPro:\n",
        "    \"\"\"Main class combining all AI methods for text expansion\"\"\"\n",
        "\n",
        "    def __init__(self, text: str):\n",
        "        self.text = text\n",
        "        self.data = None\n",
        "        self.processor = None\n",
        "\n",
        "        # Models\n",
        "        self.markov = None\n",
        "        self.word2vec_markov = None\n",
        "        self.lstm = None\n",
        "        self.gpt2 = None\n",
        "        self.similarity = None\n",
        "\n",
        "        # Flags\n",
        "        self.basic_ready = False\n",
        "        self.enhanced_ready = False\n",
        "        self.lstm_ready = False\n",
        "        self.gpt2_ready = False\n",
        "\n",
        "    def initialize_basic(self):\n",
        "        \"\"\"Initialize basic Markov Chain (fast)\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ðŸŸ¢ Initializing BASIC (Markov Chain)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Process document\n",
        "        print(\"\\nðŸ“– Processing document...\")\n",
        "        self.processor = DocumentProcessor(text=self.text)\n",
        "        self.data = self.processor.process()\n",
        "\n",
        "        print(f\"   âœ“ {self.data['sentence_count']} sentences\")\n",
        "        print(f\"   âœ“ {self.data['word_count']} words\")\n",
        "        print(f\"   âœ“ {self.data['unique_words']} unique words\")\n",
        "\n",
        "        # Train Markov\n",
        "        print(\"\\nðŸ”— Training Markov Chain...\")\n",
        "        self.markov = MarkovChain(order=2)\n",
        "        self.markov.train(self.data['sentences'])\n",
        "        print(f\"   âœ“ {len(self.markov.chain)} transitions learned\")\n",
        "\n",
        "        # Initialize similarity\n",
        "        print(\"\\nðŸ” Building similarity index...\")\n",
        "        self.similarity = EnhancedSimilarityFinder(\n",
        "            self.data['sentences'],\n",
        "            self.data['paragraphs'],\n",
        "            use_transformers=True\n",
        "        )\n",
        "\n",
        "        self.basic_ready = True\n",
        "        print(\"\\nâœ… Basic mode ready!\")\n",
        "\n",
        "    def initialize_enhanced(self):\n",
        "        \"\"\"Initialize Word2Vec enhanced Markov (medium)\"\"\"\n",
        "        if not self.basic_ready:\n",
        "            self.initialize_basic()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ðŸŸ¡ Initializing ENHANCED (Word2Vec + Markov)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        self.word2vec_markov = Word2VecMarkov(order=2)\n",
        "        self.word2vec_markov.train(self.data['sentences'], self.data['tokenized'])\n",
        "\n",
        "        self.enhanced_ready = True\n",
        "        print(\"\\nâœ… Enhanced mode ready!\")\n",
        "\n",
        "    def initialize_lstm(self, epochs: int = 20):\n",
        "        \"\"\"Initialize and train LSTM model (slower)\"\"\"\n",
        "        if not self.basic_ready:\n",
        "            self.initialize_basic()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ðŸŸ  Initializing ADVANCED (LSTM Neural Network)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        self.lstm = LSTMTextGenerator()\n",
        "        self.lstm.train(self.data['cleaned_text'], epochs=epochs)\n",
        "\n",
        "        self.lstm_ready = True\n",
        "        print(\"\\nâœ… LSTM mode ready!\")\n",
        "\n",
        "    def initialize_gpt2(self, epochs: int = 3):\n",
        "        \"\"\"Initialize and fine-tune GPT-2 (slowest, best quality)\"\"\"\n",
        "        if not self.basic_ready:\n",
        "            self.initialize_basic()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ðŸ”´ Initializing PRO (Fine-tuned GPT-2)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        self.gpt2 = GPT2TextGenerator()\n",
        "        self.gpt2.train(self.data['cleaned_text'], epochs=epochs)\n",
        "\n",
        "        self.gpt2_ready = True\n",
        "        print(\"\\nâœ… GPT-2 mode ready!\")\n",
        "\n",
        "    def expand(self, input_sentence: str, method: str = 'enhanced',\n",
        "               num_sentences: int = 4, temperature: float = 0.8) -> str:\n",
        "        \"\"\"\n",
        "        Expand input sentence into a paragraph.\n",
        "\n",
        "        Methods:\n",
        "        - 'basic': Simple Markov Chain\n",
        "        - 'enhanced': Word2Vec + Markov (recommended)\n",
        "        - 'lstm': LSTM neural network\n",
        "        - 'gpt2': Fine-tuned GPT-2 (best quality)\n",
        "        - 'hybrid': Combines multiple methods\n",
        "        \"\"\"\n",
        "        if method == 'basic':\n",
        "            return self._expand_basic(input_sentence, num_sentences)\n",
        "        elif method == 'enhanced':\n",
        "            return self._expand_enhanced(input_sentence, num_sentences)\n",
        "        elif method == 'lstm':\n",
        "            return self._expand_lstm(input_sentence, temperature)\n",
        "        elif method == 'gpt2':\n",
        "            return self._expand_gpt2(input_sentence, temperature)\n",
        "        elif method == 'hybrid':\n",
        "            return self._expand_hybrid(input_sentence, num_sentences, temperature)\n",
        "        else:\n",
        "            return f\"Unknown method: {method}\"\n",
        "\n",
        "    def _expand_basic(self, input_sentence: str, num_sentences: int) -> str:\n",
        "        \"\"\"Expand using basic Markov Chain\"\"\"\n",
        "        if not self.basic_ready:\n",
        "            return \"Basic mode not initialized. Run initialize_basic() first.\"\n",
        "\n",
        "        result = [input_sentence]\n",
        "        used = {input_sentence.lower()}\n",
        "\n",
        "        for _ in range(num_sentences - 1):\n",
        "            seed = result[-1].split()[-3:]\n",
        "            generated = self.markov.generate(seed, max_words=35)\n",
        "            if generated and generated.lower() not in used:\n",
        "                result.append(generated)\n",
        "                used.add(generated.lower())\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "    def _expand_enhanced(self, input_sentence: str, num_sentences: int) -> str:\n",
        "        \"\"\"Expand using Word2Vec enhanced Markov\"\"\"\n",
        "        if not self.enhanced_ready:\n",
        "            return \"Enhanced mode not initialized. Run initialize_enhanced() first.\"\n",
        "\n",
        "        result = [input_sentence]\n",
        "        used = {input_sentence.lower()}\n",
        "\n",
        "        # Get context from similar sentences\n",
        "        similar = self.similarity.find_similar_sentences(input_sentence, 3)\n",
        "        context_words = []\n",
        "        for sent, _ in similar:\n",
        "            context_words.extend(sent.split()[:10])\n",
        "\n",
        "        attempts = 0\n",
        "        while len(result) < num_sentences and attempts < num_sentences * 5:\n",
        "            attempts += 1\n",
        "\n",
        "            if attempts % 2 == 0 and context_words:\n",
        "                seed = random.sample(context_words, min(3, len(context_words)))\n",
        "            else:\n",
        "                seed = result[-1].split()[-3:]\n",
        "\n",
        "            generated = self.word2vec_markov.generate(seed, max_words=35)\n",
        "\n",
        "            if generated and len(generated.split()) > 4:\n",
        "                gen_lower = generated.lower()\n",
        "                is_dup = any(self._similarity_ratio(gen_lower, u) > 0.6 for u in used)\n",
        "                if not is_dup:\n",
        "                    result.append(generated)\n",
        "                    used.add(gen_lower)\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "    def _expand_lstm(self, input_sentence: str, temperature: float) -> str:\n",
        "        \"\"\"Expand using LSTM\"\"\"\n",
        "        if not self.lstm_ready:\n",
        "            return \"LSTM mode not initialized. Run initialize_lstm() first.\"\n",
        "\n",
        "        return self.lstm.generate(input_sentence, length=300, temperature=temperature)\n",
        "\n",
        "    def _expand_gpt2(self, input_sentence: str, temperature: float) -> str:\n",
        "        \"\"\"Expand using GPT-2\"\"\"\n",
        "        if not self.gpt2_ready:\n",
        "            return \"GPT-2 mode not initialized. Run initialize_gpt2() first.\"\n",
        "\n",
        "        return self.gpt2.generate(input_sentence, max_length=200, temperature=temperature)\n",
        "\n",
        "    def _expand_hybrid(self, input_sentence: str, num_sentences: int, temperature: float) -> str:\n",
        "        \"\"\"Expand using combination of methods\"\"\"\n",
        "        result = [input_sentence]\n",
        "\n",
        "        # Use enhanced if available\n",
        "        if self.enhanced_ready:\n",
        "            enhanced_result = self._expand_enhanced(input_sentence, num_sentences)\n",
        "            enhanced_sents = enhanced_result.split('. ')\n",
        "            result.extend(enhanced_sents[1:3])\n",
        "\n",
        "        # Add from GPT-2 if available\n",
        "        if self.gpt2_ready and len(result) < num_sentences:\n",
        "            gpt2_result = self._expand_gpt2(result[-1], temperature)\n",
        "            # Extract new sentences\n",
        "            gpt2_sents = gpt2_result.split('. ')\n",
        "            for sent in gpt2_sents[1:]:\n",
        "                if len(result) >= num_sentences:\n",
        "                    break\n",
        "                if sent and len(sent) > 20:\n",
        "                    result.append(sent.strip())\n",
        "\n",
        "        # Fill remaining with similarity\n",
        "        if len(result) < num_sentences and self.similarity:\n",
        "            similar = self.similarity.find_similar_sentences(input_sentence, num_sentences)\n",
        "            for sent, _ in similar:\n",
        "                if len(result) >= num_sentences:\n",
        "                    break\n",
        "                if sent not in result:\n",
        "                    result.append(sent)\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "    def _similarity_ratio(self, s1: str, s2: str) -> float:\n",
        "        \"\"\"Calculate word overlap ratio\"\"\"\n",
        "        w1 = set(s1.split())\n",
        "        w2 = set(s2.split())\n",
        "        if not w1 or not w2:\n",
        "            return 0\n",
        "        return len(w1 & w2) / min(len(w1), len(w2))\n",
        "\n",
        "    def get_status(self):\n",
        "        \"\"\"Show status of all models\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ðŸ“Š Model Status\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"ðŸŸ¢ Basic (Markov):     {'âœ… Ready' if self.basic_ready else 'âŒ Not initialized'}\")\n",
        "        print(f\"ðŸŸ¡ Enhanced (Word2Vec): {'âœ… Ready' if self.enhanced_ready else 'âŒ Not initialized'}\")\n",
        "        print(f\"ðŸŸ  Advanced (LSTM):     {'âœ… Ready' if self.lstm_ready else 'âŒ Not initialized'}\")\n",
        "        print(f\"ðŸ”´ Pro (GPT-2):         {'âœ… Ready' if self.gpt2_ready else 'âŒ Not initialized'}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… All classes defined successfully!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxufWaopVm7p"
      },
      "source": [
        "## Step 3: Upload Your Document\n",
        "\n",
        "Upload your novel/document in markdown format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEM0eKRJVm7p"
      },
      "outputs": [],
      "source": [
        "# Upload your markdown file\n",
        "print(\"ðŸ“¤ Please upload your markdown (.md) document:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file content\n",
        "document_text = \"\"\n",
        "filename = \"\"\n",
        "\n",
        "for fn, content in uploaded.items():\n",
        "    filename = fn\n",
        "    document_text = content.decode('utf-8')\n",
        "    print(f\"\\nâœ… File '{fn}' uploaded!\")\n",
        "    print(f\"   Size: {len(content):,} bytes\")\n",
        "    print(f\"   Characters: {len(document_text):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES1SJKKrVm7p"
      },
      "source": [
        "## Step 4: Initialize Text Expander\n",
        "\n",
        "Choose which models to initialize based on your needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxLtS4U-Vm7p"
      },
      "outputs": [],
      "source": [
        "# Create the Text Expander instance\n",
        "expander = TextExpanderPro(text=document_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nizAAMNVm7p"
      },
      "outputs": [],
      "source": [
        "# ðŸŸ¢ Initialize Basic Mode (Fast - ~10 seconds)\n",
        "# Always run this first!\n",
        "expander.initialize_basic()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84whwpBnVm7q"
      },
      "outputs": [],
      "source": [
        "# ðŸŸ¡ Initialize Enhanced Mode (Medium - ~30 seconds)\n",
        "# Adds Word2Vec for better word selection\n",
        "expander.initialize_enhanced()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF1CQNDQVm7q"
      },
      "outputs": [],
      "source": [
        "# ðŸŸ  Initialize LSTM Mode (Slower - ~3-5 minutes)\n",
        "# Trains a neural network on your document\n",
        "# Adjust epochs: more = better quality, longer training\n",
        "expander.initialize_lstm(epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rt84BjUVm7q"
      },
      "outputs": [],
      "source": [
        "# ðŸ”´ Initialize GPT-2 Mode (Slowest - ~10-15 minutes)\n",
        "# Fine-tunes a pre-trained language model\n",
        "# Best quality results!\n",
        "expander.initialize_gpt2(epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZvqeWhaVm7q",
        "outputId": "7c65c647-a0a5-446d-e5f8-c7bf2f090df7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸ“Š Model Status\n",
            "==================================================\n",
            "ðŸŸ¢ Basic (Markov):     âœ… Ready\n",
            "ðŸŸ¡ Enhanced (Word2Vec): âŒ Not initialized\n",
            "ðŸŸ  Advanced (LSTM):     âœ… Ready\n",
            "ðŸ”´ Pro (GPT-2):         âŒ Not initialized\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Check which models are ready\n",
        "expander.get_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcQoIcczVm7q"
      },
      "source": [
        "## Step 5: Expand Sentences! ðŸš€\n",
        "\n",
        "Now you can expand sentences using different methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rscHVv0uVm7q"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ–Šï¸ Text Expander Interface { run: \"auto\", display-mode: \"form\" }\n",
        "\n",
        "input_sentence = \"name\"  #@param {type:\"string\"}\n",
        "method = \"gpt2\"  #@param [\"basic\", \"enhanced\", \"lstm\", \"gpt2\", \"hybrid\"]\n",
        "num_sentences = 6  #@param {type:\"slider\", min:2, max:8, step:1}\n",
        "temperature = 0.7  #@param {type:\"slider\", min:0.5, max:1.5, step:0.1}\n",
        "\n",
        "print(f\"ðŸ“ Input: {input_sentence}\")\n",
        "print(f\"âš™ï¸  Method: {method}\")\n",
        "print(f\"ðŸŒ¡ï¸  Temperature: {temperature}\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "result = expander.expand(\n",
        "    input_sentence,\n",
        "    method=method,\n",
        "    num_sentences=num_sentences,\n",
        "    temperature=temperature\n",
        ")\n",
        "\n",
        "print(\"\\nðŸ“„ OUTPUT:\")\n",
        "print(\"-\"*70)\n",
        "print(textwrap.fill(result, width=70))\n",
        "print(\"-\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3JXHyIcVm7r"
      },
      "source": [
        "## ðŸ”„ Quick Functions\n",
        "\n",
        "Use these for fast text expansion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Sj4l6xpVm7r"
      },
      "outputs": [],
      "source": [
        "def expand(sentence, method='enhanced', n=4, temp=0.8):\n",
        "    \"\"\"Quick expansion function\"\"\"\n",
        "    result = expander.expand(sentence, method=method, num_sentences=n, temperature=temp)\n",
        "    print(f\"\\nðŸ“ Input: {sentence}\")\n",
        "    print(f\"âš™ï¸  Method: {method}\")\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(textwrap.fill(result, width=70))\n",
        "    print(\"-\"*70)\n",
        "    return result\n",
        "\n",
        "def compare_methods(sentence):\n",
        "    \"\"\"Compare all available methods\"\"\"\n",
        "    print(f\"\\nðŸ“ Input: {sentence}\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    methods = []\n",
        "    if expander.basic_ready:\n",
        "        methods.append(('basic', 'ðŸŸ¢ Basic'))\n",
        "    if expander.enhanced_ready:\n",
        "        methods.append(('enhanced', 'ðŸŸ¡ Enhanced'))\n",
        "    if expander.lstm_ready:\n",
        "        methods.append(('lstm', 'ðŸŸ  LSTM'))\n",
        "    if expander.gpt2_ready:\n",
        "        methods.append(('gpt2', 'ðŸ”´ GPT-2'))\n",
        "\n",
        "    for method, label in methods:\n",
        "        print(f\"\\n{label}:\")\n",
        "        print(\"-\"*70)\n",
        "        result = expander.expand(sentence, method=method, num_sentences=3)\n",
        "        print(textwrap.fill(result, width=70))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "print(\"âœ… Quick functions defined!\")\n",
        "print(\"\\nUsage:\")\n",
        "print('  expand(\"Your sentence here\")')\n",
        "print('  expand(\"Your sentence\", method=\"gpt2\", temp=0.9)')\n",
        "print('  compare_methods(\"Your sentence here\")')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rl5b0OfVm7r"
      },
      "outputs": [],
      "source": [
        "# Try it out!\n",
        "expand(\"The ancient castle stood silent in the moonlight.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV6lCYtmVm7r"
      },
      "outputs": [],
      "source": [
        "# Compare all methods\n",
        "compare_methods(\"She opened the mysterious letter with trembling hands.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffs-c962Vm7r"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ“š Method Comparison\n",
        "\n",
        "| Method | Quality | Speed | Best For |\n",
        "|--------|---------|-------|----------|\n",
        "| ðŸŸ¢ Basic | â­â­ | âš¡âš¡âš¡ | Quick testing |\n",
        "| ðŸŸ¡ Enhanced | â­â­â­ | âš¡âš¡ | Daily use |\n",
        "| ðŸŸ  LSTM | â­â­â­â­ | âš¡ | Creative generation |\n",
        "| ðŸ”´ GPT-2 | â­â­â­â­â­ | âš¡ | Best quality |\n",
        "| ðŸ”µ Hybrid | â­â­â­â­ | âš¡ | Balanced output |\n",
        "\n",
        "### Tips:\n",
        "- **Temperature**: Lower (0.5-0.7) = more focused, Higher (0.9-1.2) = more creative\n",
        "- **Start with Enhanced** - It provides good quality with reasonable speed\n",
        "- **Use GPT-2** for final/production quality output\n",
        "- **Longer documents = better results** - More training data helps all methods\n",
        "\n",
        "---"
      ]
    }
  ]
}