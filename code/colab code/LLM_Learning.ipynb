{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Menjalankan LLM dengan Colab\n",
        "\n",
        "Percobaan disini bisa digunakan juga dengan model LLM lainnya, selain deepseek.  \n",
        "\n",
        "Minimal penggunaan GPU adalah T4 GPU, ini saja sudah mepet. GPU Usage 13/15 GB.\n",
        "\n",
        "Ref:\n",
        "\n",
        "https://www.google.com/url?q=https%3A%2F%2Fmedium.com%2F%40hakimnaufal%2Ftrying-out-vllm-deepseek-r1-in-google-colab-a-quick-guide-a4fe682b8665\n",
        "\n",
        "\n",
        "Tujuan:\n",
        "- komunikasi dengan LLM\n",
        "- analisis file pdf"
      ],
      "metadata": {
        "id": "f3d_TeN2xWVh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install PIP yang dibutuhkan\n"
      ],
      "metadata": {
        "id": "FKGGKxz-x3PD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T4Npr9W4vwyB",
        "outputId": "46c567aa-dabd-41c6-f1b2-0b1626ae5df2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vllm\n",
            "  Downloading vllm-0.11.0-cp38-abi3-manylinux1_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from vllm) (2024.11.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from vllm) (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from vllm) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vllm) (4.67.1)\n",
            "Collecting blake3 (from vllm)\n",
            "  Downloading blake3-1.0.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (217 bytes)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.55.2 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.56.2)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.22.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from vllm) (5.29.5)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.118.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm) (3.12.15)\n",
            "Requirement already satisfied: openai>=1.99.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.109.1)\n",
            "Requirement already satisfied: pydantic>=2.11.7 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.11.9)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.23.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm) (11.3.0)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.11.0)\n",
            "Collecting lm-format-enforcer==0.11.3 (from vllm)\n",
            "  Downloading lm_format_enforcer-0.11.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.11 (from vllm)\n",
            "  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting outlines_core==0.2.11 (from vllm)\n",
            "  Downloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting diskcache==5.6.3 (from vllm)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting lark==1.2.2 (from vllm)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.25 (from vllm)\n",
            "  Downloading xgrammar-0.1.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.15.0)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (3.19.1)\n",
            "Collecting partial-json-parser (from vllm)\n",
            "  Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (26.2.1)\n",
            "Collecting msgspec (from vllm)\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf>=0.13.0 (from vllm)\n",
            "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting mistral_common>=1.8.2 (from mistral_common[audio,image]>=1.8.2->vllm)\n",
            "  Downloading mistral_common-1.8.5-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.12.0.88)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm) (6.0.3)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.17.0)\n",
            "Collecting setuptools<80,>=77.0.3 (from vllm)\n",
            "  Downloading setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm) (0.8.1)\n",
            "Collecting compressed-tensors==0.11.0 (from vllm)\n",
            "  Downloading compressed_tensors-0.11.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.19.0 (from vllm)\n",
            "  Downloading depyf-0.19.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm) (3.1.1)\n",
            "Collecting watchfiles (from vllm)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm) (3.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm) (1.16.2)\n",
            "Collecting ninja (from vllm)\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pybase64 (from vllm)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Collecting cbor2 (from vllm)\n",
            "  Downloading cbor2-5.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting setproctitle (from vllm)\n",
            "  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
            "Collecting openai-harmony>=0.0.3 (from vllm)\n",
            "  Downloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Collecting numba==0.61.2 (from vllm)\n",
            "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm)\n",
            "  Downloading ray-2.49.2-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision==0.23.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.23.0+cu126)\n",
            "Collecting xformers==0.0.32.post1 (from vllm)\n",
            "  Downloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.11.0->vllm) (2.4.6)\n",
            "Collecting astor (from depyf==0.19.0->vllm)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from depyf==0.19.0->vllm) (0.3.8)\n",
            "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.11.3->vllm)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lm-format-enforcer==0.11.3->vllm) (25.0)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
            "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (3.4.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.48.0)\n",
            "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading fastapi_cli-0.0.13-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.37.0)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (4.25.1)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm)\n",
            "  Downloading pydantic_extra_types-2.10.6-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm) (0.4.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (8.3.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.1)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm) (13.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2025.8.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.21.1->vllm) (0.35.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.2->vllm) (0.6.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.20.1)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.19.2)\n",
            "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading rich_toolkit-0.15.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading fastapi_cloud_cli-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.21.1->vllm) (1.1.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->vllm) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.27.1)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->vllm) (1.3.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.13.1)\n",
            "Requirement already satisfied: soxr>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (1.0.0)\n",
            "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading rignore-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.39.0)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (13.9.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.0.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.23)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
            "Downloading vllm-0.11.0-cp38-abi3-manylinux1_x86_64.whl (438.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.2/438.2 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.11.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading depyf-0.19.0-py3-none-any.whl (39 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.11.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.8.5-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading ray-2.49.2-cp312-cp312-manylinux2014_x86_64.whl (70.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-79.0.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blake3-1.0.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (387 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.9/387.9 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cbor2-5.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.9/284.9 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl (10 kB)\n",
            "Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (32 kB)\n",
            "Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
            "Downloading fastapi_cli-0.0.13-py3-none-any.whl (11 kB)\n",
            "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.6-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi_cloud_cli-0.3.1-py3-none-any.whl (19 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m131.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_toolkit-0.15.1-py3-none-any.whl (29 kB)\n",
            "Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rignore-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (951 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m951.1/951.1 kB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvloop, setuptools, setproctitle, rignore, pycountry, pybase64, partial-json-parser, outlines_core, ninja, msgspec, llvmlite, llguidance, lark, interegular, httptools, gguf, dnspython, diskcache, cbor2, blake3, astor, watchfiles, numba, email-validator, depyf, rich-toolkit, pydantic-extra-types, prometheus-fastapi-instrumentator, openai-harmony, lm-format-enforcer, xformers, ray, fastapi-cloud-cli, fastapi-cli, xgrammar, mistral_common, compressed-tensors, vllm\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: lark\n",
            "    Found existing installation: lark 1.3.0\n",
            "    Uninstalling lark-1.3.0:\n",
            "      Successfully uninstalled lark-1.3.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed astor-0.8.1 blake3-1.0.7 cbor2-5.7.0 compressed-tensors-0.11.0 depyf-0.19.0 diskcache-5.6.3 dnspython-2.8.0 email-validator-2.3.0 fastapi-cli-0.0.13 fastapi-cloud-cli-0.3.1 gguf-0.17.1 httptools-0.7.1 interegular-0.3.3 lark-1.2.2 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.11.3 mistral_common-1.8.5 msgspec-0.19.0 ninja-1.13.0 numba-0.61.2 openai-harmony-0.0.4 outlines_core-0.2.11 partial-json-parser-0.2.1.1.post6 prometheus-fastapi-instrumentator-7.1.0 pybase64-1.4.2 pycountry-24.6.1 pydantic-extra-types-2.10.6 ray-2.49.2 rich-toolkit-0.15.1 rignore-0.7.0 setproctitle-1.3.7 setuptools-79.0.1 uvloop-0.21.0 vllm-0.11.0 watchfiles-1.1.0 xformers-0.0.32.post1 xgrammar-0.1.25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "9b0a260d9b294d59aed203755eed857f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastai in /usr/local/lib/python3.12/dist-packages (2.8.4)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (from fastai) (24.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from fastai) (25.0)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from fastai) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.9,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from fastai) (1.8.12)\n",
            "Requirement already satisfied: fasttransform>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastai) (0.0.2)\n",
            "Requirement already satisfied: torchvision>=0.11 in /usr/local/lib/python3.12/dist-packages (from fastai) (0.23.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from fastai) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from fastai) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from fastai) (2.32.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from fastai) (6.0.3)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.12/dist-packages (from fastai) (1.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from fastai) (11.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from fastai) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from fastai) (1.16.2)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.12/dist-packages (from fastai) (3.8.7)\n",
            "Requirement already satisfied: plum-dispatch in /usr/local/lib/python3.12/dist-packages (from fastai) (2.5.7)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from fastai) (3.1.1)\n",
            "Requirement already satisfied: torch<2.9,>=1.10 in /usr/local/lib/python3.12/dist-packages (from fastai) (2.8.0+cu126)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (0.19.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (2.0.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (79.0.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4->fastai) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->fastai) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->fastai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->fastai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->fastai) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=1.10->fastai) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fastai) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fastai) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fastai) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fastai) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fastai) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fastai) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->fastai) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->fastai) (2025.2)\n",
            "Requirement already satisfied: beartype>=0.16.2 in /usr/local/lib/python3.12/dist-packages (from plum-dispatch->fastai) (0.21.0)\n",
            "Requirement already satisfied: rich>=10.0 in /usr/local/lib/python3.12/dist-packages (from plum-dispatch->fastai) (13.9.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->fastai) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->fastai) (3.6.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->fastai) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.0->plum-dispatch->fastai) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.0->plum-dispatch->fastai) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<2.9,>=1.10->fastai) (1.3.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4->fastai) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4->fastai) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy<4->fastai) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.0->plum-dispatch->fastai) (0.1.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4->fastai) (1.17.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install vllm # you could pass if you don't want to be prompted to restart runtime !pip install --quiet vllm\n",
        "!pip install fastai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penggunaan PIP:\n",
        "\n",
        "- FastApi: adalah python web framework untuk membuat API. Dimana disini user bisa mengirimkan data dan mendapatkan respon dari model\n",
        "\n",
        "- uvicorn: adalah ASGI ( Asynchronous Server Gateway Interface ) server. Uvicor akan melengkapi aplikasi web yang dibuat olef FastApi dan menjalankannya. Secara sederhana, FastApi adalah protokol dan Uvicorn adalah yang menjalankan protokol.\n",
        "\n",
        "- nest-asyncio: Ini adalah alat untuk membantu kita untuk menjalankan dua proses secara sekaligus. Ini nanti akan kita pakai untuk menjalankan loop dari FastAPI ataupun Uvicorn. karena mereka berdua sama-sama memiliki loop dan saling ketergantungan.\n",
        "\n",
        "- pyngrok: Ini adalah wrapper untuk ngrok. Ngrok adalah alat yang dapat membantu kita untuk mengirimkan data ke luar dari komputer, seperti di dunia internet. ini sangat membantu untuk pengembangan, jadi kita tidak perlu repot-repot menghosting aplikasi, aplikasi lokal kita sudah bisa ada di internet.\n",
        "\n",
        "- vllm: Ini adalah library python penting untuk menjalankan LLM. Keuntungan adalah meningkatkan kecepatan dan efektifitas, mendorong untuk menerima request yang lebih banyak per detik dan meningkatkan kemampuan memory dari model.\n",
        "\n",
        "- fastai: adalah library yang dibangun diatas PyTorch. ini akan menyederhakan proses training dan deploying neuiral network. Jika tadi vllm ada untuk meningkatkan penggunaan LLM inference, fastai akan menyediakan alat-alat yang kita butuhkan untuk melakukan training.\n",
        "\n",
        "\n",
        "**Tetapi** disini kita tidak akan menggunakan ngrok dan uvicorn terlebih dahulu\n",
        "\n"
      ],
      "metadata": {
        "id": "3LztUqueyQv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kamu bisa coba-coba model lain disini: https://www.google.com/url?q=https%3A%2F%2Fhuggingface.co%2Fdeepseek-ai%2FDeepSeek-R1%233-model-downloads\n",
        "\n"
      ],
      "metadata": {
        "id": "UpNDSYrm3JZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Menjalankan model di background"
      ],
      "metadata": {
        "id": "VqL_qMmV30qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Untuk menjalankan model\n",
        "import subprocess\n",
        "# model bisa diambil dari sini: https://huggingface.co/deepseek-ai/DeepSeek-R1#3-model-downloads\n",
        "model = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
        "#model = 'ibm-granite/granite-4.0-h-small'\n",
        "\n",
        "# Mulai Jalankan vllm dibagian background komputer\n",
        "vllm_process = subprocess.Popen([\n",
        "    'vllm',\n",
        "    'serve',\n",
        "    model,\n",
        "    '--trust-remote-code',\n",
        "    '--dtype', 'half',\n",
        "    '--max-model-len', '16384',\n",
        "    '--tensor-parallel-size', '1' # Subcommand akan mendeskripsikan penggunaan model\n",
        "], stdout=subprocess.PIPE, stderr=subprocess.PIPE, start_new_session=True)"
      ],
      "metadata": {
        "id": "Do11cKri3Ds0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Check dan Test vllm\n",
        "\n",
        "Prossses ini akan menjadi test pertama. kita akan mengetahui apakah vllm berjalan dengan baik jika sudah dijalankan."
      ],
      "metadata": {
        "id": "bQCjhMoT4cQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from typing import Tuple\n",
        "import sys\n",
        "\n",
        "def check_vllm_status(url: str = \"http://localhost:8000/health\") -> bool:\n",
        "    \"\"\"Untuk mencari tau apakajh LLM berfungsi normal.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        return response.status_code == 200\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        return False\n",
        "\n",
        "def monitor_vllm_process(vllm_process: subprocess.Popen, check_interval: int = 5) -> Tuple[bool, str, str]:\n",
        "    \"\"\"\n",
        "    Monitoring status vllm dan prosesnya , stdout, and stderr.\n",
        "    Returns: (success, stdout, stderr)\n",
        "    \"\"\"\n",
        "    print(\"Starting VLLM server monitoring...\")\n",
        "\n",
        "    while vllm_process.poll() is None:  # While loop selama proses masih berjalan\n",
        "        if check_vllm_status():\n",
        "            print(\"✓ VLLM server is up and running!\")\n",
        "            return True, \"\", \"\"\n",
        "\n",
        "        print(\"Waiting for VLLM server to start...\")\n",
        "        time.sleep(check_interval)\n",
        "\n",
        "        # Menampilkan Output jika ditemukan.\n",
        "        if vllm_process.stdout.readable():\n",
        "            stdout = vllm_process.stdout.read1().decode('utf-8')\n",
        "            if stdout:\n",
        "                print(\"STDOUT:\", stdout)\n",
        "\n",
        "        if vllm_process.stderr.readable():\n",
        "            stderr = vllm_process.stderr.read1().decode('utf-8')\n",
        "            if stderr:\n",
        "                print(\"STDERR:\", stderr)\n",
        "\n",
        "    # Jika sampai disini, maka proses telah selesai\n",
        "    stdout, stderr = vllm_process.communicate()\n",
        "    return False, stdout.decode('utf-8'), stderr.decode('utf-8')"
      ],
      "metadata": {
        "id": "xnl4gCAc40jK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Buat persimpangan jika VLLM sukses dan tidak"
      ],
      "metadata": {
        "id": "6DGQ2piv75qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    success, stdout, stderr = monitor_vllm_process(vllm_process)\n",
        "\n",
        "    if not success:\n",
        "        print(\"\\n❌ VLLM server failed to start!\")\n",
        "        print(\"\\nFull STDOUT:\", stdout)\n",
        "        print(\"\\nFull STDERR:\", stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n⚠️ Monitoring interrupted by user\")\n",
        "    # # This should just exited the process of probing, not the vllm, if you want it then you coul uncomment this.\n",
        "    # vllm_process.terminate()\n",
        "    # try:\n",
        "    #     vllm_process.wait(timeout=5)\n",
        "    # except subprocess.TimeoutExpired:\n",
        "    #     vllm_process.kill()\n",
        "\n",
        "    stdout, stderr = vllm_process.communicate()\n",
        "    if stdout: print(\"\\nFinal STDOUT:\", stdout.decode('utf-8'))\n",
        "    if stderr: print(\"\\nFinal STDERR:\", stderr.decode('utf-8'))\n",
        "    sys.exit(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9kEm01M72lr",
        "outputId": "2c7d0999-10f5-48a1-e64d-814047d9feae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting VLLM server monitoring...\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 10-10 11:43:53 [__init__.py:216] Automatically detected platform cuda.\n",
            "\n",
            "STDERR: 2025-10-10 11:43:55.232578: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: \u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:44:07 [api_server.py:1839] vLLM API server version 0.11.0\n",
            "\n",
            "STDERR: WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760096635.253427     743 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760096635.259439     743 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760096635.277095     743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760096635.277117     743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760096635.277118     743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760096635.277120     743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-10 11:43:55.281602: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: \u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:44:07 [utils.py:233] non-default args: {'model_tag': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'trust_remote_code': True, 'dtype': 'half', 'max_model_len': 16384}\n",
            "\n",
            "STDERR: \u001b[1;36m(APIServer pid=743)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: \u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:44:25 [model.py:547] Resolved architecture: Qwen2ForCausalLM\n",
            "\n",
            "STDERR: \u001b[1;36m(APIServer pid=743)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: \u001b[1;36m(APIServer pid=743)\u001b[0;0m WARNING 10-10 11:44:25 [model.py:1733] Casting torch.bfloat16 to torch.float16.\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:44:25 [model.py:1510] Using max model len 16384\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:44:27 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "\n",
            "STDERR: 2025-10-10 11:44:35.243792: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 10-10 11:44:33 [__init__.py:216] Automatically detected platform cuda.\n",
            "\n",
            "STDERR: WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760096675.280266     980 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760096675.290181     980 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760096675.313527     980 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760096675.313548     980 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760096675.313552     980 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760096675.313554     980 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: \u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:44:43 [core.py:644] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:44:43 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m ERROR 10-10 11:44:43 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:44:44 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m WARNING 10-10 11:44:44 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:44:44 [gpu_model_runner.py:2602] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:44:44 [gpu_model_runner.py:2634] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:44:45 [cuda.py:372] Using FlexAttention backend on V1 engine.\n",
            "\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: \u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:44:45 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:45:55 [weight_utils.py:413] Time spent downloading weights for deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B: 69.086861 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:45:55 [weight_utils.py:450] No model.safetensors.index.json found in remote.\n",
            "\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.74s/it]\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: \u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:46:06 [default_loader.py:267] Loading weights took 10.83 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:46:06 [gpu_model_runner.py:2653] Model loading took 3.3466 GiB and 81.151229 seconds\n",
            "\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.74s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m \n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: \u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:46:15 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/b55112bfd5/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:46:15 [backends.py:559] Dynamo bytecode transform time: 7.93 s\n",
            "\n",
            "STDERR: \u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m [rank0]:W1010 11:46:19.390000 980 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: \u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:46:18 [backends.py:197] Cache the graph for dynamic shape for later use\n",
            "\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: \u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:46:38 [backends.py:218] Compiling a graph for dynamic shape takes 23.16 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:46:43 [monitor.py:34] torch.compile takes 31.09 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:46:44 [gpu_worker.py:298] Available KV cache memory: 8.49 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:46:45 [kv_cache_utils.py:1087] GPU KV cache size: 318,112 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:46:45 [kv_cache_utils.py:1091] Maximum concurrency for 16,384 tokens per request: 19.42x\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m WARNING 10-10 11:46:45 [gpu_model_runner.py:3663] CUDAGraphMode.FULL_AND_PIECEWISE is not supported with FlexAttentionMetadataBuilder backend (support: AttentionCGSupport.NEVER); setting cudagraph_mode=PIECEWISE because attention is compiled piecewise\n",
            "\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:04<00:00, 13.84it/s]\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: \u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:46:51 [gpu_model_runner.py:3480] Graph capturing finished in 6 secs, took 0.43 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=980)\u001b[0;0m INFO 10-10 11:46:51 [core.py:210] init engine (profile, create kv cache, warmup model) took 44.46 seconds\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:52 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 19882\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:53 [api_server.py:1634] Supported_tasks: ['generate']\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m WARNING 10-10 11:46:53 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:53 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.95}\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:53 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.95}\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.95}\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:34] Available routes are:\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /docs, Methods: HEAD, GET\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /redoc, Methods: HEAD, GET\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /health, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /load, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /ping, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /ping, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /tokenize, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /detokenize, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /v1/models, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /version, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /v1/responses, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /v1/chat/completions, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /v1/completions, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /v1/embeddings, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /pooling, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /classify, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /score, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /v1/score, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO 10-10 11:46:54 [launcher.py:42] Route: /v1/audio/translations, Methods: POST\n",
            "\u001b[1;36m(\n",
            "STDERR: \u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO:     Started server process [743]\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO:     Waiting for application startup.\n",
            "\u001b[1;36m(APIServer pid=743)\u001b[0;0m INFO:     Application startup complete.\n",
            "\n",
            "✓ VLLM server is up and running!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Jalankan dan fungsi tambahan"
      ],
      "metadata": {
        "id": "1Jee8Fp_8XaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from fastapi.responses import StreamingResponse\n",
        "import requests\n",
        "\n",
        "# mengirimkan Request skema untuk input\n",
        "class QuestionRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "\n",
        "def ask_model(question: str):\n",
        "    \"\"\"\n",
        "    Kirimkan request ke model dan dapatkan respon.\n",
        "    \"\"\"\n",
        "    url = \"http://localhost:8000/v1/chat/completions\"  # Atur kembali jika kamu mendapati URL yang berbeda\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": question\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    response.raise_for_status()  # Kirimkan informasi jika ada error HTTP\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "9PFT_Gee8qro"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Coba gunakan modelnya\n",
        "\n"
      ],
      "metadata": {
        "id": "-OppOsps9mlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "who is first president of indonesia?\n",
        "\"\"\"\n",
        "\n",
        "# Umpamakan ask_model berasal dari cell Wt2lqQ_vfrdn tersedia\n",
        "try:\n",
        "    result = ask_model(question)\n",
        "    import json\n",
        "    print(json.dumps(result, indent=2))\n",
        "    abc = result['choices'][0]['message']['content']\n",
        "    print(abc)\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error sending request: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJXDqV0p9wfy",
        "outputId": "02596787-871f-4767-d81b-d9deb3272858"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-c9d2dbb7ebde44d5bb32a723a6648c76\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1760096828,\n",
            "  \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"Okay, so I need to figure out who the first president of Indonesia is. I remember hearing that Indonesia was formed in 1904, but I'm not exactly sure who the first president was. I think it was someone named Riaih Fani, but I'm not certain. Let me try to piece this together.\\n\\nFirst, I should probably start by recalling some basic facts about Indonesia. It's a country in Southeast Asia, known for its rich culture, especially with its traditional dance form, kaya. The government is the Indonesian People's RepUBLIC, and they have a president and a prime minister. I believe there are some political changes over the years, so maybe the first president was someone before the country was established.\\n\\nI think Indonesia was formed by merging two separate countries, Indonesia and Indonesia, in 1904. Before that, they were part of a different government structure. I recall that before Indonesia became a separate country, it was part of Indonesia, which was under a different political system. Maybe before that, Indonesia had a president from Indonesia itself, and then Indonesia was formed, which would have a different president.\\n\\nWait, I think the first president was the Prime Minister of Indonesia before it was a separate country. So perhaps it was someone like Riaih Fani, who became the first president. I'm not sure about the exact name, but I think it was Riaih Fani, the first Prime Minister. She was also the first President of Indonesia. \\n\\nI should also consider the political changes. Before Indonesia became a separate country, there were different political systems in Indonesia. There was a period before the establishment of Indonesia as a separate country where Indonesia was part of Indonesia, which was under a different system. Then, in 1904, they were formed into Indonesia, which had a new government structure. So the first president would have been someone from Indonesia before it was split.\\n\\nI think Riaih Fani was the first president, and she was also the first Prime Minister. She became the president in 1906, and she was also the first PM. So, putting it all together, Riaih Fani is the first president and the first Prime Minister of Indonesia.\\n\\nWait, but I'm a bit confused because I remember hearing that Indonesia's first president was the Prime Minister, not the President. Or maybe it's the same person. I think in Indonesia, the President and Prime Minister are the same person, but I'm not entirely sure. I should check that.\\n\\nLet me think. In Indonesia, the President is also the Prime Minister. So if Riaih Fani was the first Prime Minister, she was also the first President. But I'm not 100% certain. I should probably look up the exact details, but since I can't access the internet, I'll have to rely on my memory.\\n\\nI believe that Indonesia was formed in 1904, and Riaih Fani was the first Prime Minister and President. So she became the first president. Therefore, Riaih Fani is the first president of Indonesia.\\n</think>\\n\\nThe first president of Indonesia was Riaih Fani, who was also the first Prime Minister and PM of Indonesia. She assumed the role in 1906.\",\n",
            "        \"refusal\": null,\n",
            "        \"annotations\": null,\n",
            "        \"audio\": null,\n",
            "        \"function_call\": null,\n",
            "        \"tool_calls\": [],\n",
            "        \"reasoning_content\": null\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"stop_reason\": null,\n",
            "      \"token_ids\": null\n",
            "    }\n",
            "  ],\n",
            "  \"service_tier\": null,\n",
            "  \"system_fingerprint\": null,\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 13,\n",
            "    \"total_tokens\": 690,\n",
            "    \"completion_tokens\": 677,\n",
            "    \"prompt_tokens_details\": null\n",
            "  },\n",
            "  \"prompt_logprobs\": null,\n",
            "  \"prompt_token_ids\": null,\n",
            "  \"kv_transfer_params\": null\n",
            "}\n",
            "Okay, so I need to figure out who the first president of Indonesia is. I remember hearing that Indonesia was formed in 1904, but I'm not exactly sure who the first president was. I think it was someone named Riaih Fani, but I'm not certain. Let me try to piece this together.\n",
            "\n",
            "First, I should probably start by recalling some basic facts about Indonesia. It's a country in Southeast Asia, known for its rich culture, especially with its traditional dance form, kaya. The government is the Indonesian People's RepUBLIC, and they have a president and a prime minister. I believe there are some political changes over the years, so maybe the first president was someone before the country was established.\n",
            "\n",
            "I think Indonesia was formed by merging two separate countries, Indonesia and Indonesia, in 1904. Before that, they were part of a different government structure. I recall that before Indonesia became a separate country, it was part of Indonesia, which was under a different political system. Maybe before that, Indonesia had a president from Indonesia itself, and then Indonesia was formed, which would have a different president.\n",
            "\n",
            "Wait, I think the first president was the Prime Minister of Indonesia before it was a separate country. So perhaps it was someone like Riaih Fani, who became the first president. I'm not sure about the exact name, but I think it was Riaih Fani, the first Prime Minister. She was also the first President of Indonesia. \n",
            "\n",
            "I should also consider the political changes. Before Indonesia became a separate country, there were different political systems in Indonesia. There was a period before the establishment of Indonesia as a separate country where Indonesia was part of Indonesia, which was under a different system. Then, in 1904, they were formed into Indonesia, which had a new government structure. So the first president would have been someone from Indonesia before it was split.\n",
            "\n",
            "I think Riaih Fani was the first president, and she was also the first Prime Minister. She became the president in 1906, and she was also the first PM. So, putting it all together, Riaih Fani is the first president and the first Prime Minister of Indonesia.\n",
            "\n",
            "Wait, but I'm a bit confused because I remember hearing that Indonesia's first president was the Prime Minister, not the President. Or maybe it's the same person. I think in Indonesia, the President and Prime Minister are the same person, but I'm not entirely sure. I should check that.\n",
            "\n",
            "Let me think. In Indonesia, the President is also the Prime Minister. So if Riaih Fani was the first Prime Minister, she was also the first President. But I'm not 100% certain. I should probably look up the exact details, but since I can't access the internet, I'll have to rely on my memory.\n",
            "\n",
            "I believe that Indonesia was formed in 1904, and Riaih Fani was the first Prime Minister and President. So she became the first president. Therefore, Riaih Fani is the first president of Indonesia.\n",
            "</think>\n",
            "\n",
            "The first president of Indonesia was Riaih Fani, who was also the first Prime Minister and PM of Indonesia. She assumed the role in 1906.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dibawah ini untuk analisa PDF"
      ],
      "metadata": {
        "id": "o-5i3o0vuAg1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "897b7d40"
      },
      "source": [
        "# Task\n",
        "Implement a solution to upload a PDF file, extract its text content, and send the text to the DeepSeek model for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85a050f8"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install libraries for handling PDF files in Python.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53d202ec"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the PyMuPDF library using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8be08e60",
        "outputId": "bb64e61e-6c7e-4b5a-f3aa-f62b7417c196"
      },
      "source": [
        "!pip install PyMuPDF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c813cd2"
      },
      "source": [
        "## Upload pdf file\n",
        "\n",
        "### Subtask:\n",
        "Create a mechanism to upload a PDF file to the Colab environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a783a4b6"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary module and use the upload function to allow the user to upload a PDF file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6203e59"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3d5569e"
      },
      "source": [
        "## Integrate pdf analysis into the workflow\n",
        "\n",
        "### Subtask:\n",
        "Combine the PDF processing and model interaction steps into a cohesive workflow that the user can easily use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebe3d19e"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the main workflow function to orchestrate the PDF processing and model analysis steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab8be033"
      },
      "source": [
        "import fitz\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "from google.colab import files\n",
        "\n",
        "# Global variable to store the extracted text and model history\n",
        "global_extracted_text = \"\"\n",
        "global_message_history = []\n",
        "\n",
        "def ask_model(question: str, history: list = []):\n",
        "    \"\"\"\n",
        "    Kirimkan request ke model dan dapatkan respon, dengan mempertahankan history percakapan.\n",
        "    \"\"\"\n",
        "    url = \"http://localhost:8000/v1/chat/completions\"  # Atur kembali jika kamu mendapati URL yang berbeda\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "    # Include the history in the messages\n",
        "    messages = history + [{\"role\": \"user\", \"content\": question}]\n",
        "\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    response.raise_for_status()  # Kirimkan informasi jika ada error HTTP\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def analyze_pdf_workflow():\n",
        "    \"\"\"\n",
        "    Orchestrates the PDF analysis workflow: upload, extract, chunk, analyze, display.\n",
        "    Stores the extracted text and initial analysis in global variables for later use.\n",
        "    \"\"\"\n",
        "    global global_extracted_text\n",
        "    global global_message_history\n",
        "\n",
        "    print(\"Please upload your PDF file.\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded. Exiting.\")\n",
        "        return\n",
        "\n",
        "    for file_name, file_content in uploaded.items():\n",
        "        print(f\"Processing file: {file_name}\")\n",
        "\n",
        "        # Save the uploaded file locally\n",
        "        with open(file_name, 'wb') as f:\n",
        "            f.write(file_content)\n",
        "\n",
        "        # Extract text from PDF\n",
        "        doc = fitz.open(file_name)\n",
        "        extracted_text = \"\"\n",
        "        for page_num in range(doc.page_count):\n",
        "            page = doc.load_page(page_num)\n",
        "            extracted_text += page.get_text()\n",
        "        doc.close()\n",
        "        global_extracted_text = extracted_text # Store the extracted text globally\n",
        "        print(f\"Extracted {len(global_extracted_text)} characters from {file_name}.\")\n",
        "\n",
        "        # Create an initial message for the model with the extracted text\n",
        "        initial_message_content = f\"Here is the content of the document for analysis:\\n\\n{global_extracted_text}\"\n",
        "        # Split initial message if too long (optional, depending on model context window)\n",
        "        # For now, sending as one message\n",
        "        global_message_history = [{\"role\": \"user\", \"content\": initial_message_content}]\n",
        "\n",
        "        print(\"\\n--- Initial Analysis (if applicable) ---\")\n",
        "        # You might want to send a specific prompt for initial analysis here\n",
        "        # For now, we just store the text for subsequent interactions.\n",
        "        print(\"PDF content loaded into model's context. You can now ask questions about it.\")\n",
        "\n",
        "\n",
        "# # Call the main workflow function to execute the process\n",
        "# analyze_pdf_workflow()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah coba upload pdf diatas, coba upload yang sama disini. tujuan adalah untuk meningat isi konten dari pdf."
      ],
      "metadata": {
        "id": "l8f1uT7x4-jO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d095f723"
      },
      "source": [
        "# Run this cell to upload a PDF and load its content into the model's memory\n",
        "analyze_pdf_workflow()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d87ecc3"
      },
      "source": [
        "# After uploading the PDF, you can ask questions about it like this:\n",
        "question = \"Can you summarize the main points of the document?\"\n",
        "response = ask_model(question, history=global_message_history)\n",
        "\n",
        "# Print the model's response\n",
        "if 'choices' in response and len(response['choices']) > 0 and 'message' in response['choices'][0] and 'content' in response['choices'][0]['message']:\n",
        "    analysis_content = response['choices'][0]['message']['content']\n",
        "    print(\"Model's response:\")\n",
        "    print(analysis_content)\n",
        "else:\n",
        "    print(\"Unexpected response format:\", response)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}